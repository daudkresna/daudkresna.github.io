{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Penambangan Data \u00b6 Nama : Daud Kresna Dwiva NIM : 180411100080 Dosen Pembimbing : Mulaab","title":"Home"},{"location":"#penambangan-data","text":"Nama : Daud Kresna Dwiva NIM : 180411100080 Dosen Pembimbing : Mulaab","title":"Penambangan Data"},{"location":"Naive/","text":"Classifier adalah model machine learning yang digunakan untuk membedakan objek berdasarkan fitur tertentu. Naive Bayes Classifier adalah machine learning yang menggunakan probabilitas untuk mengklasfikasi objek Teorema Bayes : \u00b6 Tipe Data Numerik \u00b6 $$ P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} $$ Dimana (P(c)) = Prior/Probabilitas kelas dari data yang ada P(c) P(c) = Prior (Probability) P(xi) P(xi) = Evidenence dari setiap fitur/Probabilitas dari setiap fitur (P(xi|c) = Likelihood dari setiap fitur yang diperoleh dari setiap kelas dengan menggunakan rumus: $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e^{-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c})^2} $$ Tipe Data Categorical \u00b6 P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} Dimana P(xi|c) dapat diperoleh dari probabilitas berapa banyak fitur yang muncul dibagi banyak kelas yang muncul pada data yang ada Tipe Data Campuran \u00b6 Untuk tipe data campuran maka kita akan menggunakan rumus sesuai tipe dari attribut tersebut. Jika atributnya adalah numerik maka kita akan menggunakan rumus numerik akan tetapi jika attributnya categorical maka kita akan meggunakan rumus categorical from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML, display; from tabulate import tabulate def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) # IRIS TRAINING TABLE iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.9 3.2 4.8 1.8 versicolor 6.9 3.1 5.1 2.3 virginica 6.6 2.9 4.6 1.3 versicolor 6.7 2.5 5.8 1.8 virginica 5 3 1.6 0.2 setosa 4.4 3 1.3 0.2 setosa 5.7 3.8 1.7 0.3 setosa 4.8 3.4 1.9 0.2 setosa 5.4 3 4.5 1.5 versicolor 6.9 3.2 5.7 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 2.7 3.9 1.4 versicolor 5.5 3.5 1.3 0.2 setosa 5.2 4.1 1.5 0.1 setosa 6.7 3.1 4.7 1.5 versicolor 6 2.9 4.5 1.5 versicolor 4.4 3.2 1.3 0.2 setosa 5 3.5 1.6 0.6 setosa 6.5 2.8 4.6 1.5 versicolor 6 3.4 4.5 1.6 versicolor 6.4 2.7 5.3 1.9 virginica 5.9 3 5.1 1.8 virginica 4.8 3 1.4 0.3 setosa 6.4 2.8 5.6 2.2 virginica 5.7 2.6 3.5 1 versicolor 6 2.2 5 1.5 virginica 5.5 2.4 3.7 1 versicolor 6 3 4.8 1.8 virginica 5.2 3.5 1.5 0.2 setosa 7.9 3.8 6.4 2 virginica Sampel data untuk tes \u00b6 test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4] Identifikasi Per Grup Class Target untuk data Training \u00b6 dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.00909091 3.42727273 1.50909091 0.26363636] Sigma_s => [0.40361998 0.35802488 0.18683975 0.1361817 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3 1.6 0.2 setosa 4.4 3 1.3 0.2 setosa 5.7 3.8 1.7 0.3 setosa 4.8 3.4 1.9 0.2 setosa 5.1 3.7 1.5 0.4 setosa 5.5 3.5 1.3 0.2 setosa 5.2 4.1 1.5 0.1 setosa 4.4 3.2 1.3 0.2 setosa 5 3.5 1.6 0.6 setosa 4.8 3 1.4 0.3 setosa 5.2 3.5 1.5 0.2 setosa versicolor ===> Mu_s => [5.95 2.9 4.33 1.41] Sigma_s => [0.51908038 0.29439203 0.45472825 0.2514403 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.9 3.2 4.8 1.8 versicolor 6.6 2.9 4.6 1.3 versicolor 5.4 3 4.5 1.5 versicolor 5.2 2.7 3.9 1.4 versicolor 6.7 3.1 4.7 1.5 versicolor 6 2.9 4.5 1.5 versicolor 6.5 2.8 4.6 1.5 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 5.5 2.4 3.7 1 versicolor virginica ===> Mu_s => [6.56666667 2.92222222 5.42222222 1.95555556] Sigma_s => [0.62849025 0.45491147 0.49944414 0.26977357] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.9 3.1 5.1 2.3 virginica 6.7 2.5 5.8 1.8 virginica 6.9 3.2 5.7 2.3 virginica 6.4 2.7 5.3 1.9 virginica 5.9 3 5.1 1.8 virginica 6.4 2.8 5.6 2.2 virginica 6 2.2 5 1.5 virginica 6 3 4.8 1.8 virginica 7.9 3.8 6.4 2 virginica Menghitung Probabilitas Prior dan Likelihood \u00b6 def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 4.11734e-06 7.19113e-05 0.0676682 1.01259e-163 0.366667 versicolor 7.4519e-08 1.20929e-11 1.74586e-06 1.44674e-23 0.333333 virginica 6.44629e-08 2.58808e-05 5.09561e-11 4.99771e-13 0.3 Memberikan rank/urutan terhadap setiap kelas \u00b6 Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 1.27461e-35 versicolor 7.58711e-48 setosa 7.43878e-175 print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica Setelah kita sudah menghitung untuk data training kita, kita akan lakukan test lagi untuk data asli kita # ONE FUNCTION FOR CLASSIFIER def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor versicolor 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica versicolor 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica versicolor 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 144 of 150 == 96.000000 % Referensi \u00b6 https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c https://www.geeksforgeeks.org/naive-bayes-classifiers/","title":"Naive Bayes Classifier"},{"location":"Naive/#teorema-bayes","text":"","title":"Teorema Bayes :"},{"location":"Naive/#tipe-data-numerik","text":"$$ P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} $$ Dimana (P(c)) = Prior/Probabilitas kelas dari data yang ada P(c) P(c) = Prior (Probability) P(xi) P(xi) = Evidenence dari setiap fitur/Probabilitas dari setiap fitur (P(xi|c) = Likelihood dari setiap fitur yang diperoleh dari setiap kelas dengan menggunakan rumus: $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e^{-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c})^2} $$","title":"Tipe Data Numerik"},{"location":"Naive/#tipe-data-categorical","text":"P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} Dimana P(xi|c) dapat diperoleh dari probabilitas berapa banyak fitur yang muncul dibagi banyak kelas yang muncul pada data yang ada","title":"Tipe Data Categorical"},{"location":"Naive/#tipe-data-campuran","text":"Untuk tipe data campuran maka kita akan menggunakan rumus sesuai tipe dari attribut tersebut. Jika atributnya adalah numerik maka kita akan menggunakan rumus numerik akan tetapi jika attributnya categorical maka kita akan meggunakan rumus categorical from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML, display; from tabulate import tabulate def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) # IRIS TRAINING TABLE iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.9 3.2 4.8 1.8 versicolor 6.9 3.1 5.1 2.3 virginica 6.6 2.9 4.6 1.3 versicolor 6.7 2.5 5.8 1.8 virginica 5 3 1.6 0.2 setosa 4.4 3 1.3 0.2 setosa 5.7 3.8 1.7 0.3 setosa 4.8 3.4 1.9 0.2 setosa 5.4 3 4.5 1.5 versicolor 6.9 3.2 5.7 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 2.7 3.9 1.4 versicolor 5.5 3.5 1.3 0.2 setosa 5.2 4.1 1.5 0.1 setosa 6.7 3.1 4.7 1.5 versicolor 6 2.9 4.5 1.5 versicolor 4.4 3.2 1.3 0.2 setosa 5 3.5 1.6 0.6 setosa 6.5 2.8 4.6 1.5 versicolor 6 3.4 4.5 1.6 versicolor 6.4 2.7 5.3 1.9 virginica 5.9 3 5.1 1.8 virginica 4.8 3 1.4 0.3 setosa 6.4 2.8 5.6 2.2 virginica 5.7 2.6 3.5 1 versicolor 6 2.2 5 1.5 virginica 5.5 2.4 3.7 1 versicolor 6 3 4.8 1.8 virginica 5.2 3.5 1.5 0.2 setosa 7.9 3.8 6.4 2 virginica","title":"Tipe Data Campuran"},{"location":"Naive/#sampel-data-untuk-tes","text":"test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4]","title":"Sampel data untuk tes"},{"location":"Naive/#identifikasi-per-grup-class-target-untuk-data-training","text":"dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.00909091 3.42727273 1.50909091 0.26363636] Sigma_s => [0.40361998 0.35802488 0.18683975 0.1361817 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3 1.6 0.2 setosa 4.4 3 1.3 0.2 setosa 5.7 3.8 1.7 0.3 setosa 4.8 3.4 1.9 0.2 setosa 5.1 3.7 1.5 0.4 setosa 5.5 3.5 1.3 0.2 setosa 5.2 4.1 1.5 0.1 setosa 4.4 3.2 1.3 0.2 setosa 5 3.5 1.6 0.6 setosa 4.8 3 1.4 0.3 setosa 5.2 3.5 1.5 0.2 setosa versicolor ===> Mu_s => [5.95 2.9 4.33 1.41] Sigma_s => [0.51908038 0.29439203 0.45472825 0.2514403 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.9 3.2 4.8 1.8 versicolor 6.6 2.9 4.6 1.3 versicolor 5.4 3 4.5 1.5 versicolor 5.2 2.7 3.9 1.4 versicolor 6.7 3.1 4.7 1.5 versicolor 6 2.9 4.5 1.5 versicolor 6.5 2.8 4.6 1.5 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 5.5 2.4 3.7 1 versicolor virginica ===> Mu_s => [6.56666667 2.92222222 5.42222222 1.95555556] Sigma_s => [0.62849025 0.45491147 0.49944414 0.26977357] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.9 3.1 5.1 2.3 virginica 6.7 2.5 5.8 1.8 virginica 6.9 3.2 5.7 2.3 virginica 6.4 2.7 5.3 1.9 virginica 5.9 3 5.1 1.8 virginica 6.4 2.8 5.6 2.2 virginica 6 2.2 5 1.5 virginica 6 3 4.8 1.8 virginica 7.9 3.8 6.4 2 virginica","title":"Identifikasi Per Grup Class Target untuk data Training"},{"location":"Naive/#menghitung-probabilitas-prior-dan-likelihood","text":"def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 4.11734e-06 7.19113e-05 0.0676682 1.01259e-163 0.366667 versicolor 7.4519e-08 1.20929e-11 1.74586e-06 1.44674e-23 0.333333 virginica 6.44629e-08 2.58808e-05 5.09561e-11 4.99771e-13 0.3","title":"Menghitung Probabilitas Prior dan Likelihood"},{"location":"Naive/#memberikan-rankurutan-terhadap-setiap-kelas","text":"Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 1.27461e-35 versicolor 7.58711e-48 setosa 7.43878e-175 print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica Setelah kita sudah menghitung untuk data training kita, kita akan lakukan test lagi untuk data asli kita # ONE FUNCTION FOR CLASSIFIER def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor versicolor 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica versicolor 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica versicolor 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 144 of 150 == 96.000000 %","title":"Memberikan rank/urutan terhadap setiap kelas"},{"location":"Naive/#referensi","text":"https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c https://www.geeksforgeeks.org/naive-bayes-classifiers/","title":"Referensi"},{"location":"jarak-data/","text":"Menghitung Jarak antar Data tipe Numerik \u00b6 Minkowski Distance \u00b6 Minkowski Distance termasuk Euclidean Distance dan Manhattan Distance , perhitungan jarak dimana nilai m adalah bilangan asli positif dan xi dan yi adalah 2 vector dalam dimensi n Rumus untuk Minkowski Distance adalah: $$ d_{min}=(\\sum_{i=1}^n|x_{i}-y_{i}|^m)^\\frac{1}{m},m\\ge1 $$ Manhattan Distance \u00b6 Manhattan Distance agak berbeda daripada Minkowski Distance dimana nilai m=1 sama halnya dengan Minkowski Distance yang sensitif terhadap oulier Rumus untuk Manhattan Distance adalah: d_{man}=\\sum_{i=1}^{n}|x_{i}-y_{i}| d_{man}=\\sum_{i=1}^{n}|x_{i}-y_{i}| Euclidean Distance \u00b6 Euclidean Distance adalah perhitungan yang paling terkenal/banyak diketahui digunakan untuk data numerik. Ini berbeda daripada Minkowski Distance dimana nilai m = 2. Euclidean Distance berjalan baik disaat digunakan untuk datasets compact atau isolated clusters. Namun dia memiliki kelemahan yaitu jika dua vektor data tidak mempunyai atribut yang sama, mereka akan memiliki jarak yang lebih kecil dibandingan dengan pasangan data vektor yang mempuna nilai atribut yang sama Average Distance \u00b6 Mengenai kelemahan daripada Euclidean Distance , Average Distance adalah modifikasi daripada Euclidean Distance untuk memperbaiki hasil. untuk dua data x,y dalam dimensi-n, rumusnya adalah: $$ d_{ave}=\\left(\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-y_{i})^2\\right)^\\frac{1}{2} $$ Weighted Euclidean Distance \u00b6 Jika kedudukan masing masing atribut tersedia, Weighted Euclidean Distance adalah modifikasi dari Euclidean Distance . Jarak ini mempunyai rumus: $$ d_{we}=\\left(\\sum_{i=1}^n w_{i}(x_{i}-y_{i}\\right)^2)^\\frac{1}{2} $$ Dimana w adalah berat yang diberikan kepada komponen ke-i Chord Distance \u00b6 Chord Distance adalah hasil modifikasi lainnya dari Euclidean Distance untuk mengatasi kelemahan Euclidean Distance lainnya. jarak ini juga bisa digunakan untuk memecahkan masalah yang disebabkan skala pengukuran juga. Jarak ini bisa di kalkulasikan dari non-normalized data. rumus jarak ini adalah: $$ d_{chord}=\\left(2-2 \\frac{{\\sum_{i=1}^n}x_{i}y_{i}}{||x||{2}||y||{2}}\\right)^\\frac{1}{2} $$ Mahalanobis Distance \u00b6 Mahalobis Distance adalah perbedaan ke Euclidean dan Manhattan distance yaitu jarak ini terbebas dari hubungan data yang mana dua data berasal $$ d_{mah}=\\sqrt{(x-y)S^{-1}(x-y)^{T}} $$ Cosine Measure \u00b6 Persammaan Cosine kebanyakan digunakan untuk mencari kemiripan dokumen dan rumusnya adalah: $$ Cosine(x,y)=\\frac{\\sum_{i=1}^{n}x_{i}{y_{i}}}{||x||2||y||2} $$ Pearson Correlation \u00b6 Pearson Correlation Digunakan dalam clustering ekspresi data. Ukuran kemiripan menghitung kemiripan antara bentuk dari dua pola ekspresi. Rumusnya adalah: $$ Pearson(x,y)=\\frac{\\sum_{i=1}^n(x_{i}-\\mu_{x})(y_{i}-\\mu_{y})}{\\sqrt{\\sum_{i=1}^n(x_{i}-y_{i})^2}\\sqrt{\\sum_{i=1}^n(x_{i}-y_{i})^2}} $$ Dimana mu x dan mu y dimaksudkan untuk x dan y. Jarak ini juga memiliki kelemahan yaitu sensitif terhadap outliers Contoh Program menghitung jarak data dari data di internet: import pandas as pd df = pd.read_csv(\"tae.csv\",nrows=4) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } English Instructor Course Summer or Regular Class Size Class Attribut 0 1 23 3 1 19 3 1 2 15 3 1 17 3 2 1 23 3 2 49 3 3 1 5 2 2 33 3 binary=[0,3] num=[1,2,4,5] from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Binary v1-v2 0 0 0 v1-v3 0 0 0 v2-v3 0 0 0 v3-v4 0 0 0 def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(df.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(df.values.tolist()[v2][jnis[x]])**2) jmlh=jmlh+(int(df.values.tolist()[v1][jnis[x]])*int(df.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[chordDist(0,1,num)]+[0], [\"v1-v3\"]+[0]+[chordDist(0,2,num)]+[0], [\"v2-v3\"]+[0]+[chordDist(1,2,num)]+[0], [\"v3-v4\"]+[0]+[chordDist(2,3,num)]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Binary v1-v2 0 1.4132090254432463 0 v1-v3 0 1.4138230758312995 0 v2-v3 0 1.4136742257751354 0 v3-v4 0 1.413841698820558 0 BINARY def binaryDist(v1,v2,jnis): q=0 r=0 s=0 t=0 for x in range (len(jnis)): if (int(df.values.tolist()[v1][jnis[x]]))==1 and (int(df.values.tolist()[v2][jnis[x]]))==1: q=q+1 elif (int(df.values.tolist()[v1][jnis[x]]))==1 and (int(df.values.tolist()[v2][jnis[x]]))==2: r=r+1 elif (int(df.values.tolist()[v1][jnis[x]]))==2 and (int(df.values.tolist()[v2][jnis[x]]))==1: s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t)) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[chordDist(0,1,num)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[chordDist(0,2,num)]+[binaryDist(0,2,binary)], [\"v2-v3\"]+[0]+[chordDist(1,2,num)]+[binaryDist(1,2,binary)], [\"v3-v4\"]+[0]+[chordDist(2,3,num)]+[binaryDist(2,3,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Binary v1-v2 0 1.4132090254432463 0.5 v1-v3 0 1.4138230758312995 0.5 v2-v3 0 1.4136742257751354 1.0 v3-v4 0 1.413841698820558 0.0 Referensi \u00b6 https://www.researchgate.net/publication/286637899_A_Comparison_Study_on_Similarity_and_Dissimilarity_Measures_in_Clustering_Continuous_Data mulaab.github.io/datamining/memahami-data/","title":"Menghitung Jarak antar Data tipe Numerik"},{"location":"jarak-data/#menghitung-jarak-antar-data-tipe-numerik","text":"","title":"Menghitung Jarak antar Data tipe Numerik"},{"location":"jarak-data/#minkowski-distance","text":"Minkowski Distance termasuk Euclidean Distance dan Manhattan Distance , perhitungan jarak dimana nilai m adalah bilangan asli positif dan xi dan yi adalah 2 vector dalam dimensi n Rumus untuk Minkowski Distance adalah: $$ d_{min}=(\\sum_{i=1}^n|x_{i}-y_{i}|^m)^\\frac{1}{m},m\\ge1 $$","title":"Minkowski Distance"},{"location":"jarak-data/#manhattan-distance","text":"Manhattan Distance agak berbeda daripada Minkowski Distance dimana nilai m=1 sama halnya dengan Minkowski Distance yang sensitif terhadap oulier Rumus untuk Manhattan Distance adalah: d_{man}=\\sum_{i=1}^{n}|x_{i}-y_{i}| d_{man}=\\sum_{i=1}^{n}|x_{i}-y_{i}|","title":"Manhattan Distance"},{"location":"jarak-data/#euclidean-distance","text":"Euclidean Distance adalah perhitungan yang paling terkenal/banyak diketahui digunakan untuk data numerik. Ini berbeda daripada Minkowski Distance dimana nilai m = 2. Euclidean Distance berjalan baik disaat digunakan untuk datasets compact atau isolated clusters. Namun dia memiliki kelemahan yaitu jika dua vektor data tidak mempunyai atribut yang sama, mereka akan memiliki jarak yang lebih kecil dibandingan dengan pasangan data vektor yang mempuna nilai atribut yang sama","title":"Euclidean Distance"},{"location":"jarak-data/#average-distance","text":"Mengenai kelemahan daripada Euclidean Distance , Average Distance adalah modifikasi daripada Euclidean Distance untuk memperbaiki hasil. untuk dua data x,y dalam dimensi-n, rumusnya adalah: $$ d_{ave}=\\left(\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-y_{i})^2\\right)^\\frac{1}{2} $$","title":"Average Distance"},{"location":"jarak-data/#weighted-euclidean-distance","text":"Jika kedudukan masing masing atribut tersedia, Weighted Euclidean Distance adalah modifikasi dari Euclidean Distance . Jarak ini mempunyai rumus: $$ d_{we}=\\left(\\sum_{i=1}^n w_{i}(x_{i}-y_{i}\\right)^2)^\\frac{1}{2} $$ Dimana w adalah berat yang diberikan kepada komponen ke-i","title":"Weighted Euclidean Distance"},{"location":"jarak-data/#chord-distance","text":"Chord Distance adalah hasil modifikasi lainnya dari Euclidean Distance untuk mengatasi kelemahan Euclidean Distance lainnya. jarak ini juga bisa digunakan untuk memecahkan masalah yang disebabkan skala pengukuran juga. Jarak ini bisa di kalkulasikan dari non-normalized data. rumus jarak ini adalah: $$ d_{chord}=\\left(2-2 \\frac{{\\sum_{i=1}^n}x_{i}y_{i}}{||x||{2}||y||{2}}\\right)^\\frac{1}{2} $$","title":"Chord Distance"},{"location":"jarak-data/#mahalanobis-distance","text":"Mahalobis Distance adalah perbedaan ke Euclidean dan Manhattan distance yaitu jarak ini terbebas dari hubungan data yang mana dua data berasal $$ d_{mah}=\\sqrt{(x-y)S^{-1}(x-y)^{T}} $$","title":"Mahalanobis Distance"},{"location":"jarak-data/#cosine-measure","text":"Persammaan Cosine kebanyakan digunakan untuk mencari kemiripan dokumen dan rumusnya adalah: $$ Cosine(x,y)=\\frac{\\sum_{i=1}^{n}x_{i}{y_{i}}}{||x||2||y||2} $$","title":"Cosine Measure"},{"location":"jarak-data/#pearson-correlation","text":"Pearson Correlation Digunakan dalam clustering ekspresi data. Ukuran kemiripan menghitung kemiripan antara bentuk dari dua pola ekspresi. Rumusnya adalah: $$ Pearson(x,y)=\\frac{\\sum_{i=1}^n(x_{i}-\\mu_{x})(y_{i}-\\mu_{y})}{\\sqrt{\\sum_{i=1}^n(x_{i}-y_{i})^2}\\sqrt{\\sum_{i=1}^n(x_{i}-y_{i})^2}} $$ Dimana mu x dan mu y dimaksudkan untuk x dan y. Jarak ini juga memiliki kelemahan yaitu sensitif terhadap outliers Contoh Program menghitung jarak data dari data di internet: import pandas as pd df = pd.read_csv(\"tae.csv\",nrows=4) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } English Instructor Course Summer or Regular Class Size Class Attribut 0 1 23 3 1 19 3 1 2 15 3 1 17 3 2 1 23 3 2 49 3 3 1 5 2 2 33 3 binary=[0,3] num=[1,2,4,5] from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Binary v1-v2 0 0 0 v1-v3 0 0 0 v2-v3 0 0 0 v3-v4 0 0 0 def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(df.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(df.values.tolist()[v2][jnis[x]])**2) jmlh=jmlh+(int(df.values.tolist()[v1][jnis[x]])*int(df.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[chordDist(0,1,num)]+[0], [\"v1-v3\"]+[0]+[chordDist(0,2,num)]+[0], [\"v2-v3\"]+[0]+[chordDist(1,2,num)]+[0], [\"v3-v4\"]+[0]+[chordDist(2,3,num)]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Binary v1-v2 0 1.4132090254432463 0 v1-v3 0 1.4138230758312995 0 v2-v3 0 1.4136742257751354 0 v3-v4 0 1.413841698820558 0 BINARY def binaryDist(v1,v2,jnis): q=0 r=0 s=0 t=0 for x in range (len(jnis)): if (int(df.values.tolist()[v1][jnis[x]]))==1 and (int(df.values.tolist()[v2][jnis[x]]))==1: q=q+1 elif (int(df.values.tolist()[v1][jnis[x]]))==1 and (int(df.values.tolist()[v2][jnis[x]]))==2: r=r+1 elif (int(df.values.tolist()[v1][jnis[x]]))==2 and (int(df.values.tolist()[v2][jnis[x]]))==1: s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t)) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[chordDist(0,1,num)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[chordDist(0,2,num)]+[binaryDist(0,2,binary)], [\"v2-v3\"]+[0]+[chordDist(1,2,num)]+[binaryDist(1,2,binary)], [\"v3-v4\"]+[0]+[chordDist(2,3,num)]+[binaryDist(2,3,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Binary v1-v2 0 1.4132090254432463 0.5 v1-v3 0 1.4138230758312995 0.5 v2-v3 0 1.4136742257751354 1.0 v3-v4 0 1.413841698820558 0.0","title":"Pearson Correlation"},{"location":"jarak-data/#referensi","text":"https://www.researchgate.net/publication/286637899_A_Comparison_Study_on_Similarity_and_Dissimilarity_Measures_in_Clustering_Continuous_Data mulaab.github.io/datamining/memahami-data/","title":"Referensi"},{"location":"memahami-data/","text":"Memahami Data \u00b6 Ada banyak jenis data dan masing masing jenisnya membutuhkan tools dan teknik yang berbeda beda Data terbagi menjadi beberapa jenis yaitu Data Terstruktur Data tidak terstruktur Data bahasa alami Machine Generated Data Data audio visual Data Streaming Data berbentuk Graph Type Data Atribut \u00b6 Atribut Biner \u00b6 termasuk nominal karena hanya kategori 0 dan 1 \u200b ada 2 bentuk : Atribut simetris \u00b6 jika keduanya memiliki nilai yang sama (contoh: Tes Urin, Tes Buta Warna (Positif dan Negatif)) \u200b Atribut Asimetris jika hasil dari nilai tidak sama pentingnya (contoh: hasil tes medis) Atribut Ordinal \u00b6 adalah atribut dengan nilai yang memliki arti urutan atau peringkat tapi besarnya nilai berurutan tersebut tidak diketahui (Contoh: Sangat puas, cukup puas, tidak puas) \u200b Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median (nilai tengah) Atribut Numerik \u00b6 bersifat kuantitatif terukur dinyatakan dengan bilangan bulat atau nilai riel Program untuk menampilkan statistik deskriptif dari kumpulan data: from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv('Tinggi.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Tinggi (cm) Berat Badan (kg) Panjang Kaki (size) Umur (tahun) 0 178 109 45 18 1 163 84 42 20 2 184 79 40 20 3 163 82 42 21 4 168 122 42 18 5 178 83 39 20 6 167 108 43 18 7 177 119 44 18 8 175 71 43 19 9 181 114 37 21 10 183 93 38 18 11 171 71 45 20 12 178 116 42 20 13 176 107 41 21 14 169 88 41 20 15 160 101 37 22 16 172 76 43 20 17 181 87 39 19 18 164 75 44 22 19 171 115 41 20 20 181 107 44 18 21 176 100 45 18 22 167 81 37 19 23 163 116 43 21 24 176 96 45 20 25 179 114 36 20 26 180 72 45 22 27 176 78 42 21 28 178 86 44 20 29 168 107 45 21 ... ... ... ... ... 70 179 121 45 21 71 166 96 43 19 72 161 88 43 21 73 176 98 36 19 74 176 115 38 21 75 178 114 38 19 76 178 124 41 21 77 173 103 36 18 78 180 124 36 20 79 167 100 40 20 80 168 97 39 18 81 160 108 36 18 82 170 119 43 18 83 162 86 37 20 84 178 95 45 18 85 171 104 44 21 86 170 96 38 18 87 185 98 37 20 88 163 93 39 18 89 178 86 39 19 90 168 71 36 20 91 166 114 38 21 92 179 76 43 20 93 171 124 43 18 94 178 71 42 22 95 174 104 43 21 96 162 92 41 22 97 165 89 39 20 98 163 82 42 22 99 170 88 42 19 100 rows \u00d7 4 columns from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"describe()\"]+['<pre>'+str(df[col].describe())+'</pre>' for col in df.columns], [\"count()\"]+[df[col].count() for col in df.columns], [\"mean()\"]+[df[col].mean() for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3()\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew()\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) method Tinggi (cm) Berat Badan (kg) Panjang Kaki (size) Umur (tahun) describe() count 100.000000 mean 172.040000 std 7.315295 min 160.000000 25% 165.000000 50% 172.000000 75% 178.000000 max 185.000000 Name: Tinggi (cm), dtype: float64 count 100.000000 mean 96.960000 std 15.966455 min 71.000000 25% 83.750000 50% 97.000000 75% 109.000000 max 124.000000 Name: Berat Badan (kg), dtype: float64 count 100.000000 mean 40.790000 std 2.857897 min 36.000000 25% 38.000000 50% 41.000000 75% 43.000000 max 45.000000 Name: Panjang Kaki (size), dtype: float64 count 100.000000 mean 19.860000 std 1.325888 min 18.000000 25% 19.000000 50% 20.000000 75% 21.000000 max 22.000000 Name: Umur (tahun), dtype: float64 count() 100 100 100 100 mean() 172.04 96.96 40.79 19.86 std() 7.32 15.97 2.86 1.33 min() 160 71 36 18 max() 185 124 45 22 q1() 165.0 83.75 38.0 19.0 q2() 172.0 97.0 41.0 20.0 q3() 178.0 109.0 43.0 21.0 skew() -0.02 -0.03 -0.25 -0.11 Mean \u00b6 Mean atau yang bisa disebut rata rata adalah jumlah data dibagi dengan banyaknya data Modus \u00b6 Data yang paling banyak keluar/menonjol dari kumpulan data Median \u00b6 Nilai tengah dari banyaknya data Quartil \u00b6 Membagi data menjadi sama banyak yang dibatasi oleh suatu nilai Kuartil bawah ( Q1 ) Kuartil tengah ( Q2 ) Kuartil atas ( Q3 ) Skewness \u00b6 Suatu ketidakseimbangan dan asimetris mean dari distribusi data, jika data itu normal maka saat didistribusikan menggunakan bell curve,maka kurvanya akan simetris","title":"Memahami data"},{"location":"memahami-data/#memahami-data","text":"Ada banyak jenis data dan masing masing jenisnya membutuhkan tools dan teknik yang berbeda beda Data terbagi menjadi beberapa jenis yaitu Data Terstruktur Data tidak terstruktur Data bahasa alami Machine Generated Data Data audio visual Data Streaming Data berbentuk Graph","title":"Memahami Data"},{"location":"memahami-data/#type-data-atribut","text":"","title":"Type Data Atribut"},{"location":"memahami-data/#atribut-biner","text":"termasuk nominal karena hanya kategori 0 dan 1 \u200b ada 2 bentuk :","title":"Atribut Biner"},{"location":"memahami-data/#atribut-simetris","text":"jika keduanya memiliki nilai yang sama (contoh: Tes Urin, Tes Buta Warna (Positif dan Negatif)) \u200b Atribut Asimetris jika hasil dari nilai tidak sama pentingnya (contoh: hasil tes medis)","title":"Atribut simetris"},{"location":"memahami-data/#atribut-ordinal","text":"adalah atribut dengan nilai yang memliki arti urutan atau peringkat tapi besarnya nilai berurutan tersebut tidak diketahui (Contoh: Sangat puas, cukup puas, tidak puas) \u200b Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median (nilai tengah)","title":"Atribut Ordinal"},{"location":"memahami-data/#atribut-numerik","text":"bersifat kuantitatif terukur dinyatakan dengan bilangan bulat atau nilai riel Program untuk menampilkan statistik deskriptif dari kumpulan data: from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv('Tinggi.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Tinggi (cm) Berat Badan (kg) Panjang Kaki (size) Umur (tahun) 0 178 109 45 18 1 163 84 42 20 2 184 79 40 20 3 163 82 42 21 4 168 122 42 18 5 178 83 39 20 6 167 108 43 18 7 177 119 44 18 8 175 71 43 19 9 181 114 37 21 10 183 93 38 18 11 171 71 45 20 12 178 116 42 20 13 176 107 41 21 14 169 88 41 20 15 160 101 37 22 16 172 76 43 20 17 181 87 39 19 18 164 75 44 22 19 171 115 41 20 20 181 107 44 18 21 176 100 45 18 22 167 81 37 19 23 163 116 43 21 24 176 96 45 20 25 179 114 36 20 26 180 72 45 22 27 176 78 42 21 28 178 86 44 20 29 168 107 45 21 ... ... ... ... ... 70 179 121 45 21 71 166 96 43 19 72 161 88 43 21 73 176 98 36 19 74 176 115 38 21 75 178 114 38 19 76 178 124 41 21 77 173 103 36 18 78 180 124 36 20 79 167 100 40 20 80 168 97 39 18 81 160 108 36 18 82 170 119 43 18 83 162 86 37 20 84 178 95 45 18 85 171 104 44 21 86 170 96 38 18 87 185 98 37 20 88 163 93 39 18 89 178 86 39 19 90 168 71 36 20 91 166 114 38 21 92 179 76 43 20 93 171 124 43 18 94 178 71 42 22 95 174 104 43 21 96 162 92 41 22 97 165 89 39 20 98 163 82 42 22 99 170 88 42 19 100 rows \u00d7 4 columns from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"describe()\"]+['<pre>'+str(df[col].describe())+'</pre>' for col in df.columns], [\"count()\"]+[df[col].count() for col in df.columns], [\"mean()\"]+[df[col].mean() for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3()\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew()\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) method Tinggi (cm) Berat Badan (kg) Panjang Kaki (size) Umur (tahun) describe() count 100.000000 mean 172.040000 std 7.315295 min 160.000000 25% 165.000000 50% 172.000000 75% 178.000000 max 185.000000 Name: Tinggi (cm), dtype: float64 count 100.000000 mean 96.960000 std 15.966455 min 71.000000 25% 83.750000 50% 97.000000 75% 109.000000 max 124.000000 Name: Berat Badan (kg), dtype: float64 count 100.000000 mean 40.790000 std 2.857897 min 36.000000 25% 38.000000 50% 41.000000 75% 43.000000 max 45.000000 Name: Panjang Kaki (size), dtype: float64 count 100.000000 mean 19.860000 std 1.325888 min 18.000000 25% 19.000000 50% 20.000000 75% 21.000000 max 22.000000 Name: Umur (tahun), dtype: float64 count() 100 100 100 100 mean() 172.04 96.96 40.79 19.86 std() 7.32 15.97 2.86 1.33 min() 160 71 36 18 max() 185 124 45 22 q1() 165.0 83.75 38.0 19.0 q2() 172.0 97.0 41.0 20.0 q3() 178.0 109.0 43.0 21.0 skew() -0.02 -0.03 -0.25 -0.11","title":"Atribut Numerik"},{"location":"memahami-data/#mean","text":"Mean atau yang bisa disebut rata rata adalah jumlah data dibagi dengan banyaknya data","title":"Mean"},{"location":"memahami-data/#modus","text":"Data yang paling banyak keluar/menonjol dari kumpulan data","title":"Modus"},{"location":"memahami-data/#median","text":"Nilai tengah dari banyaknya data","title":"Median"},{"location":"memahami-data/#quartil","text":"Membagi data menjadi sama banyak yang dibatasi oleh suatu nilai Kuartil bawah ( Q1 ) Kuartil tengah ( Q2 ) Kuartil atas ( Q3 )","title":"Quartil"},{"location":"memahami-data/#skewness","text":"Suatu ketidakseimbangan dan asimetris mean dari distribusi data, jika data itu normal maka saat didistribusikan menggunakan bell curve,maka kurvanya akan simetris","title":"Skewness"},{"location":"seleksi-fitur/","text":"Seleksi Fitur adalah kegiatan untuk memilih / menyaring / memberi peringkat terhadap tiap tiap fitur dalam data agar nantinya fitur yang memiliki peringkat rendah dapat dibuang Seleksi Fitur sangat berguna dalam Data Mining karena dapat menghemat waktu, biaya, dan tenaga dengan cara membuang fitur yang tidak penting. jika kita menganalisa banyak kolom dengan kita menyeleksi fitur data tersebut kita dapat mengurangi kolom data itu sehingga membuat waktu analisis lebih cepat Information Gain \u00b6 Ada banyak teknik untuk menyeleksi fitur. Information Gain bekerja dengan mendeteksi fitur yang paling banyak memiliki informasi berdasarkan kelas tertentu dengan menghitung nilai Entropy-nya Rumus untuk mencari Entropy Target adalah: $$ Entropy(S):\\sum_{i}^{c}-P_{i}\\log_{2}P_{i} $$ Dimana c adalah jumlah nilai yang ada pada kelas kategori dan Pi adalah jumlah sampel untuk kelas i from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) df = read_csv('newdata.csv', sep=';') table(df) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no Mencari Entropy Target \u00b6 def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('entropy target =', entropyTarget) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309 Dari atas kita tahu bahwa Entropy Target adalah hasil dari penjumlahan Entropy kelas no dan yes Kemudian untuk mencari Entropy setiap fitur / atribut dalam data digunakan rumus: $$ Gain(S,A)=Entropy(S)-\\sum_{values(A)}\\frac{|S_{v}|}{|S|}Entropy(S_{v}) $$ A adalah Atribut dan |Sv| adalah jumlah sampel untuk nilai v, |S| adalah jumlah seluruh sampel serta Entropy(Sv) adalah Entropy dari sampel yang memiliki nilai v Rumus ini digunakan untuk semua kolom def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"gain of\",column,\"is\",gain) return gain gains = [[x,findGain(x)] for x in ['outlook','temperature','humidity','windy']] value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927 Hasil daripada setiap kolomnya: table(DataFrame(gains, columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 Setelah mengetahui hasil dari setiap kolom, maka akan terlihat mana kolom yang memiliki hasil tertinggi dan terendah. Karena kita sudah mengetahui kolom dengan nilai yang terendah adalah temperatur maka tidak apa apa untuk menghilangkan kolom temperatur dari data Referensi \u00b6 https://edrianhadinata.wordpress.com/tag/reduksi-dimensi/ https://www.researchgate.net/publication/326571453_Seleksi_Fitur_Information_Gain_untuk_Klasifikasi_Penyakit_Jantung_Menggunakan_Kombinasi_Metode_K-Nearest_Neighbor_dan_Naive_Bayes https://informatikalogi.com/algoritma-id3","title":"Seleksi Fitur"},{"location":"seleksi-fitur/#information-gain","text":"Ada banyak teknik untuk menyeleksi fitur. Information Gain bekerja dengan mendeteksi fitur yang paling banyak memiliki informasi berdasarkan kelas tertentu dengan menghitung nilai Entropy-nya Rumus untuk mencari Entropy Target adalah: $$ Entropy(S):\\sum_{i}^{c}-P_{i}\\log_{2}P_{i} $$ Dimana c adalah jumlah nilai yang ada pada kelas kategori dan Pi adalah jumlah sampel untuk kelas i from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) df = read_csv('newdata.csv', sep=';') table(df) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no","title":"Information Gain"},{"location":"seleksi-fitur/#mencari-entropy-target","text":"def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('entropy target =', entropyTarget) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309 Dari atas kita tahu bahwa Entropy Target adalah hasil dari penjumlahan Entropy kelas no dan yes Kemudian untuk mencari Entropy setiap fitur / atribut dalam data digunakan rumus: $$ Gain(S,A)=Entropy(S)-\\sum_{values(A)}\\frac{|S_{v}|}{|S|}Entropy(S_{v}) $$ A adalah Atribut dan |Sv| adalah jumlah sampel untuk nilai v, |S| adalah jumlah seluruh sampel serta Entropy(Sv) adalah Entropy dari sampel yang memiliki nilai v Rumus ini digunakan untuk semua kolom def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"gain of\",column,\"is\",gain) return gain gains = [[x,findGain(x)] for x in ['outlook','temperature','humidity','windy']] value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927 Hasil daripada setiap kolomnya: table(DataFrame(gains, columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 Setelah mengetahui hasil dari setiap kolom, maka akan terlihat mana kolom yang memiliki hasil tertinggi dan terendah. Karena kita sudah mengetahui kolom dengan nilai yang terendah adalah temperatur maka tidak apa apa untuk menghilangkan kolom temperatur dari data","title":"Mencari Entropy Target"},{"location":"seleksi-fitur/#referensi","text":"https://edrianhadinata.wordpress.com/tag/reduksi-dimensi/ https://www.researchgate.net/publication/326571453_Seleksi_Fitur_Information_Gain_untuk_Klasifikasi_Penyakit_Jantung_Menggunakan_Kombinasi_Metode_K-Nearest_Neighbor_dan_Naive_Bayes https://informatikalogi.com/algoritma-id3","title":"Referensi"},{"location":"soal-naive-bayes/","text":"Teorema Bayes : \u00b6 Tipe Data Numerik \u00b6 $$ P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} $$ Dimana (P(c)) = Prior/Probabilitas kelas dari data yang ada P(c) P(c) = Prior (Probability) P(xi) P(xi) = Evidenence dari setiap fitur/Probabilitas dari setiap fitur (P(xi|c) = Likelihood dari setiap fitur yang diperoleh dari setiap kelas dengan menggunakan rumus: $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e^{-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c})^2} $$ Tipe Data Categorical \u00b6 P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} Dimana P(xi|c) dapat diperoleh dari probabilitas berapa banyak fitur yang muncul dibagi banyak kelas yang muncul pada data yang ada Tipe Data Campuran \u00b6 Untuk tipe data campuran maka kita akan menggunakan rumus sesuai tipe dari attribut tersebut. Jika atributnya adalah numerik maka kita akan menggunakan rumus numerik akan tetapi jika attributnya categorical maka kita akan meggunakan rumus categorical Contoh Soal: outlook temperature humidity windy play sunny 85 high FALSE no sunny 80 high TRUE no overcast 83 high FALSE yes rainy 70 high FALSE yes rainy 68 normal FALSE yes rainy 65 normal TRUE no overcast 64 normal TRUE yes sunny 72 high FALSE no sunny 69 normal FALSE yes rainy 75 normal FALSE yes sunny 75 normal TRUE yes overcast 72 high TRUE yes overcast 81 normal FALSE yes rainy 71 high TRUE no rainy 60 high FALSE ??? Untuk menentukan/menebak kelas dari data yang baru kita akan memakai teknik naive bayes. Dan didalam data baru tersebut terdapat atribut categorical dan atribut numerik maka untuk atribut categorical kita menggunakan probabilitas atribut itu muncul dalam setiap kelas dan untuk data numerik kita menggunakan rumus : $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e^{-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c})^2} $$ untuk menghitung rata rata dan standart deviasi dari atribut numerik 60 kita bisa menggunakan excel dan memperoleh hasil Rata - Rata = 72,66667 Standard Deviasi = 7,237469 P(60|C) = 4.027899533 untuk probabilitas kelas YES yaitu : 9/14 = 0,642857 untuk probabilitas kelas NO yaitu : 5/14 = 0,357143 untuk fitur categorical Rainy Yes : 3/9 = 0,333333 No : 2/5 = 0,4 Humidity Yes : 3/9 = 0,333333 No : 4/5 = 0,8 False Yes : 6/9 = 0,666667 No : 2/5 = 0,4 P(Yes|x) = 0.1918044093 P(No|x) = 0.1841326237 Maka kita bisa menebak bahwa data baru tersebut memiliki kelas YES outlook temperature humidity windy play rainy 60 high FALSE yes","title":"Soal Naive Bayes"},{"location":"soal-naive-bayes/#teorema-bayes","text":"","title":"Teorema Bayes :"},{"location":"soal-naive-bayes/#tipe-data-numerik","text":"$$ P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} $$ Dimana (P(c)) = Prior/Probabilitas kelas dari data yang ada P(c) P(c) = Prior (Probability) P(xi) P(xi) = Evidenence dari setiap fitur/Probabilitas dari setiap fitur (P(xi|c) = Likelihood dari setiap fitur yang diperoleh dari setiap kelas dengan menggunakan rumus: $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e^{-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c})^2} $$","title":"Tipe Data Numerik"},{"location":"soal-naive-bayes/#tipe-data-categorical","text":"P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} Dimana P(xi|c) dapat diperoleh dari probabilitas berapa banyak fitur yang muncul dibagi banyak kelas yang muncul pada data yang ada","title":"Tipe Data Categorical"},{"location":"soal-naive-bayes/#tipe-data-campuran","text":"Untuk tipe data campuran maka kita akan menggunakan rumus sesuai tipe dari attribut tersebut. Jika atributnya adalah numerik maka kita akan menggunakan rumus numerik akan tetapi jika attributnya categorical maka kita akan meggunakan rumus categorical Contoh Soal: outlook temperature humidity windy play sunny 85 high FALSE no sunny 80 high TRUE no overcast 83 high FALSE yes rainy 70 high FALSE yes rainy 68 normal FALSE yes rainy 65 normal TRUE no overcast 64 normal TRUE yes sunny 72 high FALSE no sunny 69 normal FALSE yes rainy 75 normal FALSE yes sunny 75 normal TRUE yes overcast 72 high TRUE yes overcast 81 normal FALSE yes rainy 71 high TRUE no rainy 60 high FALSE ??? Untuk menentukan/menebak kelas dari data yang baru kita akan memakai teknik naive bayes. Dan didalam data baru tersebut terdapat atribut categorical dan atribut numerik maka untuk atribut categorical kita menggunakan probabilitas atribut itu muncul dalam setiap kelas dan untuk data numerik kita menggunakan rumus : $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e^{-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c})^2} $$ untuk menghitung rata rata dan standart deviasi dari atribut numerik 60 kita bisa menggunakan excel dan memperoleh hasil Rata - Rata = 72,66667 Standard Deviasi = 7,237469 P(60|C) = 4.027899533 untuk probabilitas kelas YES yaitu : 9/14 = 0,642857 untuk probabilitas kelas NO yaitu : 5/14 = 0,357143 untuk fitur categorical Rainy Yes : 3/9 = 0,333333 No : 2/5 = 0,4 Humidity Yes : 3/9 = 0,333333 No : 4/5 = 0,8 False Yes : 6/9 = 0,666667 No : 2/5 = 0,4 P(Yes|x) = 0.1918044093 P(No|x) = 0.1841326237 Maka kita bisa menebak bahwa data baru tersebut memiliki kelas YES outlook temperature humidity windy play rainy 60 high FALSE yes","title":"Tipe Data Campuran"}]}