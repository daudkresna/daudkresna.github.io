{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Blog Statis \u00b6 Nama : Daud Kresna Dwiva NIM : 180411100080 Universitas Trunojoyo Madura","title":"Home"},{"location":"#blog-statis","text":"Nama : Daud Kresna Dwiva NIM : 180411100080 Universitas Trunojoyo Madura","title":"Blog Statis"},{"location":"Naive/","text":"Classifier adalah model machine learning yang digunakan untuk membedakan objek berdasarkan fitur tertentu. Naive Bayes Classifier adalah machine learning yang menggunakan probabilitas untuk mengklasfikasi objek Teorema Bayes : \u00b6 Tipe Data Numerik \u00b6 $$ P(c|x) = \\frac{P(x_i|c) P\u00a9}{P(x_i)} $$ Dimana (P\u00a9) = Prior/Probabilitas kelas dari data yang ada P(c) P(c) = Prior (Probability) P(xi) P(xi) = Evidenence dari setiap fitur/Probabilitas dari setiap fitur (P(xi|c) = Likelihood dari setiap fitur yang diperoleh dari setiap kelas dengan menggunakan rumus: $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e {-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c}) 2} $$ Tipe Data Categorical \u00b6 P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} Dimana P(xi|c) dapat diperoleh dari probabilitas berapa banyak fitur yang muncul dibagi banyak kelas yang muncul pada data yang ada Tipe Data Campuran \u00b6 Untuk tipe data campuran maka kita akan menggunakan rumus sesuai tipe dari attribut tersebut. Jika atributnya adalah numerik maka kita akan menggunakan rumus numerik akan tetapi jika attributnya categorical maka kita akan meggunakan rumus categorical from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML , display ; from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) # IRIS TRAINING TABLE iris = datasets . load_iris () data = [ list ( s ) + [ iris . target_names [ iris . target [ i ]]] for i , s in enumerate ( iris . data )] dataset = DataFrame ( data , columns = iris . feature_names + [ 'class' ]) . sample ( frac = 0.2 ) table ( dataset ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.9 3.2 4.8 1.8 versicolor 6.9 3.1 5.1 2.3 virginica 6.6 2.9 4.6 1.3 versicolor 6.7 2.5 5.8 1.8 virginica 5 3 1.6 0.2 setosa 4.4 3 1.3 0.2 setosa 5.7 3.8 1.7 0.3 setosa 4.8 3.4 1.9 0.2 setosa 5.4 3 4.5 1.5 versicolor 6.9 3.2 5.7 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 2.7 3.9 1.4 versicolor 5.5 3.5 1.3 0.2 setosa 5.2 4.1 1.5 0.1 setosa 6.7 3.1 4.7 1.5 versicolor 6 2.9 4.5 1.5 versicolor 4.4 3.2 1.3 0.2 setosa 5 3.5 1.6 0.6 setosa 6.5 2.8 4.6 1.5 versicolor 6 3.4 4.5 1.6 versicolor 6.4 2.7 5.3 1.9 virginica 5.9 3 5.1 1.8 virginica 4.8 3 1.4 0.3 setosa 6.4 2.8 5.6 2.2 virginica 5.7 2.6 3.5 1 versicolor 6 2.2 5 1.5 virginica 5.5 2.4 3.7 1 versicolor 6 3 4.8 1.8 virginica 5.2 3.5 1.5 0.2 setosa 7.9 3.8 6.4 2 virginica Sampel data untuk tes \u00b6 test = [ 3 , 5 , 2 , 4 ] print ( \"sampel data: \" , test ) sampel data: [3, 5, 2, 4] Identifikasi Per Grup Class Target untuk data Training \u00b6 dataset_classes = {} # table per classes for key , group in dataset . groupby ( 'class' ): mu_s = [ group [ c ] . mean () for c in group . columns [: - 1 ]] sigma_s = [ group [ c ] . std () for c in group . columns [: - 1 ]] dataset_classes [ key ] = [ group , mu_s , sigma_s ] print ( key , \"===>\" ) print ( 'Mu_s =>' , array ( mu_s )) print ( 'Sigma_s =>' , array ( sigma_s )) table ( group ) setosa ===> Mu_s => [5.00909091 3.42727273 1.50909091 0.26363636] Sigma_s => [0.40361998 0.35802488 0.18683975 0.1361817 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3 1.6 0.2 setosa 4.4 3 1.3 0.2 setosa 5.7 3.8 1.7 0.3 setosa 4.8 3.4 1.9 0.2 setosa 5.1 3.7 1.5 0.4 setosa 5.5 3.5 1.3 0.2 setosa 5.2 4.1 1.5 0.1 setosa 4.4 3.2 1.3 0.2 setosa 5 3.5 1.6 0.6 setosa 4.8 3 1.4 0.3 setosa 5.2 3.5 1.5 0.2 setosa versicolor ===> Mu_s => [5.95 2.9 4.33 1.41] Sigma_s => [0.51908038 0.29439203 0.45472825 0.2514403 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.9 3.2 4.8 1.8 versicolor 6.6 2.9 4.6 1.3 versicolor 5.4 3 4.5 1.5 versicolor 5.2 2.7 3.9 1.4 versicolor 6.7 3.1 4.7 1.5 versicolor 6 2.9 4.5 1.5 versicolor 6.5 2.8 4.6 1.5 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 5.5 2.4 3.7 1 versicolor virginica ===> Mu_s => [6.56666667 2.92222222 5.42222222 1.95555556] Sigma_s => [0.62849025 0.45491147 0.49944414 0.26977357] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.9 3.1 5.1 2.3 virginica 6.7 2.5 5.8 1.8 virginica 6.9 3.2 5.7 2.3 virginica 6.4 2.7 5.3 1.9 virginica 5.9 3 5.1 1.8 virginica 6.4 2.8 5.6 2.2 virginica 6 2.2 5 1.5 virginica 6 3 4.8 1.8 virginica 7.9 3.8 6.4 2 virginica Menghitung Probabilitas Prior dan Likelihood \u00b6 def numericalPriorProbability ( v , mu , sigma ): return ( 1.0 / sqrt ( 2 * pi * ( sigma ** 2 )) * exp ( - (( v - mu ) ** 2 ) / ( 2 * ( sigma ** 2 )))) def categoricalProbability ( sample , universe ): return sample . shape [ 0 ] / universe . shape [ 0 ] Ps = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( test )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) table ( DataFrame ( Ps , columns = [ \"classes\" ] + [ \"P( %d | C )\" % d for d in test ] + [ \"P( C )\" ])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 4.11734e-06 7.19113e-05 0.0676682 1.01259e-163 0.366667 versicolor 7.4519e-08 1.20929e-11 1.74586e-06 1.44674e-23 0.333333 virginica 6.44629e-08 2.58808e-05 5.09561e-11 4.99771e-13 0.3 Memberikan rank/urutan terhadap setiap kelas \u00b6 Pss = ([[ r [ 0 ], prod ( r [ 1 :])] for r in Ps ]) PDss = DataFrame ( Pss , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] table ( PDss ) class probability virginica 1.27461e-35 versicolor 7.58711e-48 setosa 7.43878e-175 print ( \"Prediksi Bayes untuk\" , test , \"adalah\" , PDss . values [ 0 , 0 ]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica Setelah kita sudah menghitung untuk data training kita, kita akan lakukan test lagi untuk data asli kita # ONE FUNCTION FOR CLASSIFIER def predict ( sampel ): priorLikehoods = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( sampel )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) products = ([[ r [ 0 ], prod ( r [ 1 :])] for r in priorLikehoods ]) result = DataFrame ( products , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] return result . values [ 0 , 0 ] dataset_test = DataFrame ([ list ( d ) + [ predict ( d [: 4 ])] for d in data ], columns = list ( dataset . columns ) + [ 'predicted class (by predict())' ]) table ( dataset_test ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor versicolor 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica versicolor 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica versicolor 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica corrects = dataset_test . loc [ dataset_test [ 'class' ] == dataset_test [ 'predicted class (by predict())' ]] . shape [ 0 ] print ( 'Prediksi Training Bayes: %d of %d == %f %% ' % ( corrects , len ( data ), corrects / len ( data ) * 100 )) Prediksi Training Bayes: 144 of 150 == 96.000000 % Referensi \u00b6 https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c https://www.geeksforgeeks.org/naive-bayes-classifiers/","title":"Naive Bayes Classifier"},{"location":"Naive/#teorema-bayes","text":"","title":"Teorema Bayes :"},{"location":"Naive/#tipe-data-numerik","text":"$$ P(c|x) = \\frac{P(x_i|c) P\u00a9}{P(x_i)} $$ Dimana (P\u00a9) = Prior/Probabilitas kelas dari data yang ada P(c) P(c) = Prior (Probability) P(xi) P(xi) = Evidenence dari setiap fitur/Probabilitas dari setiap fitur (P(xi|c) = Likelihood dari setiap fitur yang diperoleh dari setiap kelas dengan menggunakan rumus: $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e {-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c}) 2} $$","title":"Tipe Data Numerik"},{"location":"Naive/#tipe-data-categorical","text":"P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} Dimana P(xi|c) dapat diperoleh dari probabilitas berapa banyak fitur yang muncul dibagi banyak kelas yang muncul pada data yang ada","title":"Tipe Data Categorical"},{"location":"Naive/#tipe-data-campuran","text":"Untuk tipe data campuran maka kita akan menggunakan rumus sesuai tipe dari attribut tersebut. Jika atributnya adalah numerik maka kita akan menggunakan rumus numerik akan tetapi jika attributnya categorical maka kita akan meggunakan rumus categorical from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML , display ; from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) # IRIS TRAINING TABLE iris = datasets . load_iris () data = [ list ( s ) + [ iris . target_names [ iris . target [ i ]]] for i , s in enumerate ( iris . data )] dataset = DataFrame ( data , columns = iris . feature_names + [ 'class' ]) . sample ( frac = 0.2 ) table ( dataset ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.9 3.2 4.8 1.8 versicolor 6.9 3.1 5.1 2.3 virginica 6.6 2.9 4.6 1.3 versicolor 6.7 2.5 5.8 1.8 virginica 5 3 1.6 0.2 setosa 4.4 3 1.3 0.2 setosa 5.7 3.8 1.7 0.3 setosa 4.8 3.4 1.9 0.2 setosa 5.4 3 4.5 1.5 versicolor 6.9 3.2 5.7 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 2.7 3.9 1.4 versicolor 5.5 3.5 1.3 0.2 setosa 5.2 4.1 1.5 0.1 setosa 6.7 3.1 4.7 1.5 versicolor 6 2.9 4.5 1.5 versicolor 4.4 3.2 1.3 0.2 setosa 5 3.5 1.6 0.6 setosa 6.5 2.8 4.6 1.5 versicolor 6 3.4 4.5 1.6 versicolor 6.4 2.7 5.3 1.9 virginica 5.9 3 5.1 1.8 virginica 4.8 3 1.4 0.3 setosa 6.4 2.8 5.6 2.2 virginica 5.7 2.6 3.5 1 versicolor 6 2.2 5 1.5 virginica 5.5 2.4 3.7 1 versicolor 6 3 4.8 1.8 virginica 5.2 3.5 1.5 0.2 setosa 7.9 3.8 6.4 2 virginica","title":"Tipe Data Campuran"},{"location":"Naive/#sampel-data-untuk-tes","text":"test = [ 3 , 5 , 2 , 4 ] print ( \"sampel data: \" , test ) sampel data: [3, 5, 2, 4]","title":"Sampel data untuk tes"},{"location":"Naive/#identifikasi-per-grup-class-target-untuk-data-training","text":"dataset_classes = {} # table per classes for key , group in dataset . groupby ( 'class' ): mu_s = [ group [ c ] . mean () for c in group . columns [: - 1 ]] sigma_s = [ group [ c ] . std () for c in group . columns [: - 1 ]] dataset_classes [ key ] = [ group , mu_s , sigma_s ] print ( key , \"===>\" ) print ( 'Mu_s =>' , array ( mu_s )) print ( 'Sigma_s =>' , array ( sigma_s )) table ( group ) setosa ===> Mu_s => [5.00909091 3.42727273 1.50909091 0.26363636] Sigma_s => [0.40361998 0.35802488 0.18683975 0.1361817 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3 1.6 0.2 setosa 4.4 3 1.3 0.2 setosa 5.7 3.8 1.7 0.3 setosa 4.8 3.4 1.9 0.2 setosa 5.1 3.7 1.5 0.4 setosa 5.5 3.5 1.3 0.2 setosa 5.2 4.1 1.5 0.1 setosa 4.4 3.2 1.3 0.2 setosa 5 3.5 1.6 0.6 setosa 4.8 3 1.4 0.3 setosa 5.2 3.5 1.5 0.2 setosa versicolor ===> Mu_s => [5.95 2.9 4.33 1.41] Sigma_s => [0.51908038 0.29439203 0.45472825 0.2514403 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.9 3.2 4.8 1.8 versicolor 6.6 2.9 4.6 1.3 versicolor 5.4 3 4.5 1.5 versicolor 5.2 2.7 3.9 1.4 versicolor 6.7 3.1 4.7 1.5 versicolor 6 2.9 4.5 1.5 versicolor 6.5 2.8 4.6 1.5 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 5.5 2.4 3.7 1 versicolor virginica ===> Mu_s => [6.56666667 2.92222222 5.42222222 1.95555556] Sigma_s => [0.62849025 0.45491147 0.49944414 0.26977357] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.9 3.1 5.1 2.3 virginica 6.7 2.5 5.8 1.8 virginica 6.9 3.2 5.7 2.3 virginica 6.4 2.7 5.3 1.9 virginica 5.9 3 5.1 1.8 virginica 6.4 2.8 5.6 2.2 virginica 6 2.2 5 1.5 virginica 6 3 4.8 1.8 virginica 7.9 3.8 6.4 2 virginica","title":"Identifikasi Per Grup Class Target untuk data Training"},{"location":"Naive/#menghitung-probabilitas-prior-dan-likelihood","text":"def numericalPriorProbability ( v , mu , sigma ): return ( 1.0 / sqrt ( 2 * pi * ( sigma ** 2 )) * exp ( - (( v - mu ) ** 2 ) / ( 2 * ( sigma ** 2 )))) def categoricalProbability ( sample , universe ): return sample . shape [ 0 ] / universe . shape [ 0 ] Ps = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( test )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) table ( DataFrame ( Ps , columns = [ \"classes\" ] + [ \"P( %d | C )\" % d for d in test ] + [ \"P( C )\" ])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 4.11734e-06 7.19113e-05 0.0676682 1.01259e-163 0.366667 versicolor 7.4519e-08 1.20929e-11 1.74586e-06 1.44674e-23 0.333333 virginica 6.44629e-08 2.58808e-05 5.09561e-11 4.99771e-13 0.3","title":"Menghitung Probabilitas Prior dan Likelihood"},{"location":"Naive/#memberikan-rankurutan-terhadap-setiap-kelas","text":"Pss = ([[ r [ 0 ], prod ( r [ 1 :])] for r in Ps ]) PDss = DataFrame ( Pss , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] table ( PDss ) class probability virginica 1.27461e-35 versicolor 7.58711e-48 setosa 7.43878e-175 print ( \"Prediksi Bayes untuk\" , test , \"adalah\" , PDss . values [ 0 , 0 ]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica Setelah kita sudah menghitung untuk data training kita, kita akan lakukan test lagi untuk data asli kita # ONE FUNCTION FOR CLASSIFIER def predict ( sampel ): priorLikehoods = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( sampel )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) products = ([[ r [ 0 ], prod ( r [ 1 :])] for r in priorLikehoods ]) result = DataFrame ( products , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] return result . values [ 0 , 0 ] dataset_test = DataFrame ([ list ( d ) + [ predict ( d [: 4 ])] for d in data ], columns = list ( dataset . columns ) + [ 'predicted class (by predict())' ]) table ( dataset_test ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor versicolor 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica versicolor 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica versicolor 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica corrects = dataset_test . loc [ dataset_test [ 'class' ] == dataset_test [ 'predicted class (by predict())' ]] . shape [ 0 ] print ( 'Prediksi Training Bayes: %d of %d == %f %% ' % ( corrects , len ( data ), corrects / len ( data ) * 100 )) Prediksi Training Bayes: 144 of 150 == 96.000000 %","title":"Memberikan rank/urutan terhadap setiap kelas"},{"location":"Naive/#referensi","text":"https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c https://www.geeksforgeeks.org/naive-bayes-classifiers/","title":"Referensi"},{"location":"fuzzy-c-means/","text":"Algoritma FCM (Fuzzy C-Means) Clustering adalah salah satu algoritma yang digunakan dalam pengolahan citra. . Algoritma ini merupakan penggabungan dari Algoritma Fuzzy Logic dan Algoritma K-Means Clustering. K-Means Clustering adalah salah satu algoritma klasifikasi data yang cukup banyak dipakai untuk memecahkan masalah. Hanya saja metode tersebut tidak memiliki nilai pengembalian berupa sebuah nilai pembanding untuk masing-masing cluster, sehingga digunakan algoritma Fuzzy untuk menghitung skor dari sebuah data. Contoh Penggunaan FCM pada data: 1. Persiapkan Environment \u00b6 import pandas as pd from pandas import DataFrame import random import numpy as np from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) Data = pd . read_csv ( 'data1.csv' , sep = ',' ) Data = Data [[ 'O-Ring' , 'Thermal' , 'Temperature' , 'Leak' , 'Temporal' ]] . sample ( 6 , random_state = 42 ) D = Data . values print ( \"Table (D) >>\" ) table ( D ) Table (D) >> 0 1 2 3 4 6 0 75 200 16 6 1 63 200 10 6 0 66 50 1 6 1 57 200 9 6 0 81 200 18 6 0 67 200 13 n , m , c , w , T , e , P0 , t = * D . shape , 3 , 2 , 10 , 0.1 , 0 , 1 print ( \"Variables >>\" ) print ( \" n = %d \\n m = %d \\n c = %d \\n w = %d \\n T = %d \\n e = %f \\n P0 = %d \\n t = %d \" % ( n , m , c , w , T , e , P0 , t )) Dimana n = Jumlah Sampel m = Jumlah Fitur c = Jumlah Cluster w =Tingkat blur/fuzzy T = Batas maks Iterasi e = Akurasi Pt = Fungsi Objektif ke-1 t = Iterasi ke-t Variables >> n = 6 m = 5 c = 3 w = 2 T = 10 e = 0.100000 P0 = 0 t = 1 2. Membuat Matriks derajat cluster \u00b6 random . seed ( 42 ) U = np . array ([[ random . uniform ( 0 , 1 ) for _ in range ( c )] for _ in range ( n )]) print ( \"U >> \\n \" ) print ( U ) U >> [[0.6394268 0.02501076 0.27502932] [0.22321074 0.73647121 0.67669949] [0.89217957 0.08693883 0.42192182] [0.02979722 0.21863797 0.50535529] [0.02653597 0.19883765 0.64988444] [0.54494148 0.22044062 0.58926568]] 3. Menghitung Pusat Cluster \u00b6 # Caution: NP Array is math-agnostic (column-by-column) def cluster ( U , D , x , y ): return sum ([ U [ i , y ] ** w * D [ i , x ] for i in range ( n )]) / sum ([ U [ i , y ] ** w for i in range ( n )]) V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) print ( \"V >> \\n \" ) print ( V ) V >> [[6.00000000e+00 3.26487360e-02 6.84657455e+01 1.23129308e+02 7.54380404e+00] [6.00000000e+00 8.59703678e-01 6.39459452e+01 1.98348517e+02 1.05098259e+01] [6.00000000e+00 4.10760713e-01 6.81254962e+01 1.84623123e+02 1.17372391e+01]] 4. Hitung Fungsi Objektif pada t \u00b6 def objective ( V , U , D ): return sum ([ sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) * ( U [ i , k ] ** w ) for k in range ( c )]) for i in range ( n )]) Pt = objective ( V , U , D ) print ( \"Pt >> \\n \" ) print ( Pt ) Pt >> 12771.30605980444 5. Hitung Ulang Matrik Derajat Cluster \u00b6 def converge ( V , D , i , k ): return ( sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) ** ( - 1 / ( w - 1 ))) / sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) ** ( - 1 / ( w - 1 )) for k in range ( c )]) print ( \"U >> \\n \" ) np . array ([[ converge ( V , D , i , k ) for k in range ( c )] for i in range ( n )]) U >> array([[1.67779651e-02, 6.48642159e-01, 3.34579876e-01], [6.46312425e-04, 9.84911124e-01, 1.44425634e-02], [6.49354017e-01, 1.58552395e-01, 1.92093588e-01], [7.64136468e-03, 8.66887418e-01, 1.25471217e-01], [3.06668583e-02, 5.40465577e-01, 4.28867564e-01], [2.95350488e-03, 9.23775417e-01, 7.32710785e-02]]) 6. Cek apakah sudah berhenti atau loop kembali \u00b6 def iterate ( U ): V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) return np . array ([[ converge ( V , D , i , k ) for k in range ( c )] for i in range ( n )]), objective ( V , U , D ) def fuzzyCM ( U ): #U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) U , P2 , P , t = * iterate ( U ), 0 , 1 while abs ( P2 - P ) > e and t < T : U , P2 , P , t = * iterate ( U ), P2 , t + 1 return U , t FuzzyResult , FuzzyIters = fuzzyCM ( U ) print ( \"Iterating %d times, fuzzy result >> \\n \" % FuzzyIters ) print ( FuzzyResult ) Iterating 7 times, fuzzy result >> [[3.65865187e-04 4.10626890e-02 9.58571446e-01] [6.31837603e-05 9.94611632e-01 5.32518423e-03] [1.00000000e+00 8.69545397e-12 8.58646829e-12] [1.09520345e-03 9.48952481e-01 4.99523153e-02] [4.81513773e-04 2.62180565e-02 9.73300430e-01] [1.16981231e-03 7.98268525e-01 2.00561663e-01]] 7. Ambil Nilai Terbesar pada kolom sebagai cluster \u00b6 table ( DataFrame ([ D [ i ] . tolist () + [ np . argmax ( FuzzyResult [ i ] . tolist ())] for i in range ( n )], columns = Data . columns . tolist () + [ \"Cluster Index\" ])) O-Ring Thermal Temperature Leak Temporal Cluster Index 6 0 75 200 16 2 6 1 63 200 10 1 6 0 66 50 1 0 6 1 57 200 9 1 6 0 81 200 18 2 6 0 67 200 13 1","title":"Fuzzy C Means"},{"location":"fuzzy-c-means/#1-persiapkan-environment","text":"import pandas as pd from pandas import DataFrame import random import numpy as np from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) Data = pd . read_csv ( 'data1.csv' , sep = ',' ) Data = Data [[ 'O-Ring' , 'Thermal' , 'Temperature' , 'Leak' , 'Temporal' ]] . sample ( 6 , random_state = 42 ) D = Data . values print ( \"Table (D) >>\" ) table ( D ) Table (D) >> 0 1 2 3 4 6 0 75 200 16 6 1 63 200 10 6 0 66 50 1 6 1 57 200 9 6 0 81 200 18 6 0 67 200 13 n , m , c , w , T , e , P0 , t = * D . shape , 3 , 2 , 10 , 0.1 , 0 , 1 print ( \"Variables >>\" ) print ( \" n = %d \\n m = %d \\n c = %d \\n w = %d \\n T = %d \\n e = %f \\n P0 = %d \\n t = %d \" % ( n , m , c , w , T , e , P0 , t )) Dimana n = Jumlah Sampel m = Jumlah Fitur c = Jumlah Cluster w =Tingkat blur/fuzzy T = Batas maks Iterasi e = Akurasi Pt = Fungsi Objektif ke-1 t = Iterasi ke-t Variables >> n = 6 m = 5 c = 3 w = 2 T = 10 e = 0.100000 P0 = 0 t = 1","title":"1. Persiapkan Environment"},{"location":"fuzzy-c-means/#2-membuat-matriks-derajat-cluster","text":"random . seed ( 42 ) U = np . array ([[ random . uniform ( 0 , 1 ) for _ in range ( c )] for _ in range ( n )]) print ( \"U >> \\n \" ) print ( U ) U >> [[0.6394268 0.02501076 0.27502932] [0.22321074 0.73647121 0.67669949] [0.89217957 0.08693883 0.42192182] [0.02979722 0.21863797 0.50535529] [0.02653597 0.19883765 0.64988444] [0.54494148 0.22044062 0.58926568]]","title":"2. Membuat Matriks derajat cluster"},{"location":"fuzzy-c-means/#3-menghitung-pusat-cluster","text":"# Caution: NP Array is math-agnostic (column-by-column) def cluster ( U , D , x , y ): return sum ([ U [ i , y ] ** w * D [ i , x ] for i in range ( n )]) / sum ([ U [ i , y ] ** w for i in range ( n )]) V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) print ( \"V >> \\n \" ) print ( V ) V >> [[6.00000000e+00 3.26487360e-02 6.84657455e+01 1.23129308e+02 7.54380404e+00] [6.00000000e+00 8.59703678e-01 6.39459452e+01 1.98348517e+02 1.05098259e+01] [6.00000000e+00 4.10760713e-01 6.81254962e+01 1.84623123e+02 1.17372391e+01]]","title":"3. Menghitung Pusat Cluster"},{"location":"fuzzy-c-means/#4-hitung-fungsi-objektif-pada-t","text":"def objective ( V , U , D ): return sum ([ sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) * ( U [ i , k ] ** w ) for k in range ( c )]) for i in range ( n )]) Pt = objective ( V , U , D ) print ( \"Pt >> \\n \" ) print ( Pt ) Pt >> 12771.30605980444","title":"4. Hitung Fungsi Objektif pada t"},{"location":"fuzzy-c-means/#5-hitung-ulang-matrik-derajat-cluster","text":"def converge ( V , D , i , k ): return ( sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) ** ( - 1 / ( w - 1 ))) / sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) ** ( - 1 / ( w - 1 )) for k in range ( c )]) print ( \"U >> \\n \" ) np . array ([[ converge ( V , D , i , k ) for k in range ( c )] for i in range ( n )]) U >> array([[1.67779651e-02, 6.48642159e-01, 3.34579876e-01], [6.46312425e-04, 9.84911124e-01, 1.44425634e-02], [6.49354017e-01, 1.58552395e-01, 1.92093588e-01], [7.64136468e-03, 8.66887418e-01, 1.25471217e-01], [3.06668583e-02, 5.40465577e-01, 4.28867564e-01], [2.95350488e-03, 9.23775417e-01, 7.32710785e-02]])","title":"5. Hitung Ulang Matrik Derajat Cluster"},{"location":"fuzzy-c-means/#6-cek-apakah-sudah-berhenti-atau-loop-kembali","text":"def iterate ( U ): V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) return np . array ([[ converge ( V , D , i , k ) for k in range ( c )] for i in range ( n )]), objective ( V , U , D ) def fuzzyCM ( U ): #U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) U , P2 , P , t = * iterate ( U ), 0 , 1 while abs ( P2 - P ) > e and t < T : U , P2 , P , t = * iterate ( U ), P2 , t + 1 return U , t FuzzyResult , FuzzyIters = fuzzyCM ( U ) print ( \"Iterating %d times, fuzzy result >> \\n \" % FuzzyIters ) print ( FuzzyResult ) Iterating 7 times, fuzzy result >> [[3.65865187e-04 4.10626890e-02 9.58571446e-01] [6.31837603e-05 9.94611632e-01 5.32518423e-03] [1.00000000e+00 8.69545397e-12 8.58646829e-12] [1.09520345e-03 9.48952481e-01 4.99523153e-02] [4.81513773e-04 2.62180565e-02 9.73300430e-01] [1.16981231e-03 7.98268525e-01 2.00561663e-01]]","title":"6. Cek apakah sudah berhenti atau loop kembali"},{"location":"fuzzy-c-means/#7-ambil-nilai-terbesar-pada-kolom-sebagai-cluster","text":"table ( DataFrame ([ D [ i ] . tolist () + [ np . argmax ( FuzzyResult [ i ] . tolist ())] for i in range ( n )], columns = Data . columns . tolist () + [ \"Cluster Index\" ])) O-Ring Thermal Temperature Leak Temporal Cluster Index 6 0 75 200 16 2 6 1 63 200 10 1 6 0 66 50 1 0 6 1 57 200 9 1 6 0 81 200 18 2 6 0 67 200 13 1","title":"7. Ambil Nilai Terbesar pada kolom sebagai cluster"},{"location":"jarak-data/","text":"Menghitung Jarak antar Data tipe Numerik \u00b6 Minkowski Distance \u00b6 Minkowski Distance termasuk Euclidean Distance dan Manhattan Distance , perhitungan jarak dimana nilai m adalah bilangan asli positif dan xi dan yi adalah 2 vector dalam dimensi n Rumus untuk Minkowski Distance adalah: $$ d_{min}=(\\sum_{i=1} n|x_{i}-y_{i}| m)^\\frac{1}{m},m\\ge1 $$ Manhattan Distance \u00b6 Manhattan Distance agak berbeda daripada Minkowski Distance dimana nilai m=1 sama halnya dengan Minkowski Distance yang sensitif terhadap oulier Rumus untuk Manhattan Distance adalah: d_{man}=\\sum_{i=1}^{n}|x_{i}-y_{i}| d_{man}=\\sum_{i=1}^{n}|x_{i}-y_{i}| Euclidean Distance \u00b6 Euclidean Distance adalah perhitungan yang paling terkenal/banyak diketahui digunakan untuk data numerik. Ini berbeda daripada Minkowski Distance dimana nilai m = 2. Euclidean Distance berjalan baik disaat digunakan untuk datasets compact atau isolated clusters. Namun dia memiliki kelemahan yaitu jika dua vektor data tidak mempunyai atribut yang sama, mereka akan memiliki jarak yang lebih kecil dibandingan dengan pasangan data vektor yang mempuna nilai atribut yang sama Average Distance \u00b6 Mengenai kelemahan daripada Euclidean Distance , Average Distance adalah modifikasi daripada Euclidean Distance untuk memperbaiki hasil. untuk dua data x,y dalam dimensi-n, rumusnya adalah: $$ d_{ave}=\\left(\\frac{1}{n}\\sum_{i=1} {n}(x_{i}-y_{i}) 2\\right)^\\frac{1}{2} $$ Weighted Euclidean Distance \u00b6 Jika kedudukan masing masing atribut tersedia, Weighted Euclidean Distance adalah modifikasi dari Euclidean Distance . Jarak ini mempunyai rumus: $$ d_{we}=\\left(\\sum_{i=1}^n w_{i}(x_{i}-y_{i}\\right) 2) \\frac{1}{2} $$ Dimana w adalah berat yang diberikan kepada komponen ke-i Chord Distance \u00b6 Chord Distance adalah hasil modifikasi lainnya dari Euclidean Distance untuk mengatasi kelemahan Euclidean Distance lainnya. jarak ini juga bisa digunakan untuk memecahkan masalah yang disebabkan skala pengukuran juga. Jarak ini bisa di kalkulasikan dari non-normalized data. rumus jarak ini adalah: $$ d_{chord}=\\left(2-2 \\frac{{\\sum_{i=1} n}x_{i}y_{i}}{||x||{2}||y||{2}}\\right) \\frac{1}{2} $$ Mahalanobis Distance \u00b6 Mahalobis Distance adalah perbedaan ke Euclidean dan Manhattan distance yaitu jarak ini terbebas dari hubungan data yang mana dua data berasal $$ d_{mah}=\\sqrt{(x-y)S {-1}(x-y) {T}} $$ Cosine Measure \u00b6 Persammaan Cosine kebanyakan digunakan untuk mencari kemiripan dokumen dan rumusnya adalah: $$ Cosine(x,y)=\\frac{\\sum_{i=1}^{n}x_{i}{y_{i}}}{||x||2||y||2} $$ Pearson Correlation \u00b6 Pearson Correlation Digunakan dalam clustering ekspresi data. Ukuran kemiripan menghitung kemiripan antara bentuk dari dua pola ekspresi. Rumusnya adalah: $$ Pearson(x,y)=\\frac{\\sum_{i=1} n(x_{i}-\\mu_{x})(y_{i}-\\mu_{y})}{\\sqrt{\\sum_{i=1} n(x_{i}-y_{i}) 2}\\sqrt{\\sum_{i=1} n(x_{i}-y_{i})^2}} $$ Dimana mu x dan mu y dimaksudkan untuk x dan y. Jarak ini juga memiliki kelemahan yaitu sensitif terhadap outliers Contoh Program menghitung jarak data dari data di internet: import pandas as pd df = pd . read_csv ( \"tae.csv\" , nrows = 4 ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } English Instructor Course Summer or Regular Class Size Class Attribut 0 1 23 3 1 19 3 1 2 15 3 1 17 3 2 1 23 3 2 49 3 3 1 5 2 2 33 3 binary = [ 0 , 3 ] num = [ 1 , 2 , 4 , 5 ] from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ 0 ] + [ 0 ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Binary v1-v2 0 0 0 v1-v3 0 0 0 v2-v3 0 0 0 v3-v4 0 0 0 def chordDist ( v1 , v2 , jnis ): jmlh = 0 normv1 = 0 normv2 = 0 for x in range ( len ( jnis )): normv1 = normv1 + ( int ( df . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) normv2 = normv2 + ( int ( df . values . tolist ()[ v2 ][ jnis [ x ]]) ** 2 ) jmlh = jmlh + ( int ( df . values . tolist ()[ v1 ][ jnis [ x ]]) * int ( df . values . tolist ()[ v2 ][ jnis [ x ]])) return (( 2 - ( 2 * jmlh / ( normv1 * normv2 ))) ** 0.5 ) from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ chordDist ( 0 , 1 , num )] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ chordDist ( 0 , 2 , num )] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ chordDist ( 1 , 2 , num )] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ chordDist ( 2 , 3 , num )] + [ 0 ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Binary v1-v2 0 1.4132090254432463 0 v1-v3 0 1.4138230758312995 0 v2-v3 0 1.4136742257751354 0 v3-v4 0 1.413841698820558 0 BINARY def binaryDist ( v1 , v2 , jnis ): q = 0 r = 0 s = 0 t = 0 for x in range ( len ( jnis )): if ( int ( df . values . tolist ()[ v1 ][ jnis [ x ]])) == 1 and ( int ( df . values . tolist ()[ v2 ][ jnis [ x ]])) == 1 : q = q + 1 elif ( int ( df . values . tolist ()[ v1 ][ jnis [ x ]])) == 1 and ( int ( df . values . tolist ()[ v2 ][ jnis [ x ]])) == 2 : r = r + 1 elif ( int ( df . values . tolist ()[ v1 ][ jnis [ x ]])) == 2 and ( int ( df . values . tolist ()[ v2 ][ jnis [ x ]])) == 1 : s = s + 1 else : t = t + 1 return (( r + s ) / ( q + r + s + t )) from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ chordDist ( 0 , 1 , num )] + [ binaryDist ( 0 , 1 , binary )], [ \"v1-v3\" ] + [ 0 ] + [ chordDist ( 0 , 2 , num )] + [ binaryDist ( 0 , 2 , binary )], [ \"v2-v3\" ] + [ 0 ] + [ chordDist ( 1 , 2 , num )] + [ binaryDist ( 1 , 2 , binary )], [ \"v3-v4\" ] + [ 0 ] + [ chordDist ( 2 , 3 , num )] + [ binaryDist ( 2 , 3 , binary )], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Binary v1-v2 0 1.4132090254432463 0.5 v1-v3 0 1.4138230758312995 0.5 v2-v3 0 1.4136742257751354 1.0 v3-v4 0 1.413841698820558 0.0 Referensi \u00b6 https://www.researchgate.net/publication/286637899_A_Comparison_Study_on_Similarity_and_Dissimilarity_Measures_in_Clustering_Continuous_Data mulaab.github.io/datamining/memahami-data/","title":"Menghitung Jarak antar Data tipe Numerik"},{"location":"jarak-data/#menghitung-jarak-antar-data-tipe-numerik","text":"","title":"Menghitung Jarak antar Data tipe Numerik"},{"location":"jarak-data/#minkowski-distance","text":"Minkowski Distance termasuk Euclidean Distance dan Manhattan Distance , perhitungan jarak dimana nilai m adalah bilangan asli positif dan xi dan yi adalah 2 vector dalam dimensi n Rumus untuk Minkowski Distance adalah: $$ d_{min}=(\\sum_{i=1} n|x_{i}-y_{i}| m)^\\frac{1}{m},m\\ge1 $$","title":"Minkowski Distance"},{"location":"jarak-data/#manhattan-distance","text":"Manhattan Distance agak berbeda daripada Minkowski Distance dimana nilai m=1 sama halnya dengan Minkowski Distance yang sensitif terhadap oulier Rumus untuk Manhattan Distance adalah: d_{man}=\\sum_{i=1}^{n}|x_{i}-y_{i}| d_{man}=\\sum_{i=1}^{n}|x_{i}-y_{i}|","title":"Manhattan Distance"},{"location":"jarak-data/#euclidean-distance","text":"Euclidean Distance adalah perhitungan yang paling terkenal/banyak diketahui digunakan untuk data numerik. Ini berbeda daripada Minkowski Distance dimana nilai m = 2. Euclidean Distance berjalan baik disaat digunakan untuk datasets compact atau isolated clusters. Namun dia memiliki kelemahan yaitu jika dua vektor data tidak mempunyai atribut yang sama, mereka akan memiliki jarak yang lebih kecil dibandingan dengan pasangan data vektor yang mempuna nilai atribut yang sama","title":"Euclidean Distance"},{"location":"jarak-data/#average-distance","text":"Mengenai kelemahan daripada Euclidean Distance , Average Distance adalah modifikasi daripada Euclidean Distance untuk memperbaiki hasil. untuk dua data x,y dalam dimensi-n, rumusnya adalah: $$ d_{ave}=\\left(\\frac{1}{n}\\sum_{i=1} {n}(x_{i}-y_{i}) 2\\right)^\\frac{1}{2} $$","title":"Average Distance"},{"location":"jarak-data/#weighted-euclidean-distance","text":"Jika kedudukan masing masing atribut tersedia, Weighted Euclidean Distance adalah modifikasi dari Euclidean Distance . Jarak ini mempunyai rumus: $$ d_{we}=\\left(\\sum_{i=1}^n w_{i}(x_{i}-y_{i}\\right) 2) \\frac{1}{2} $$ Dimana w adalah berat yang diberikan kepada komponen ke-i","title":"Weighted Euclidean Distance"},{"location":"jarak-data/#chord-distance","text":"Chord Distance adalah hasil modifikasi lainnya dari Euclidean Distance untuk mengatasi kelemahan Euclidean Distance lainnya. jarak ini juga bisa digunakan untuk memecahkan masalah yang disebabkan skala pengukuran juga. Jarak ini bisa di kalkulasikan dari non-normalized data. rumus jarak ini adalah: $$ d_{chord}=\\left(2-2 \\frac{{\\sum_{i=1} n}x_{i}y_{i}}{||x||{2}||y||{2}}\\right) \\frac{1}{2} $$","title":"Chord Distance"},{"location":"jarak-data/#mahalanobis-distance","text":"Mahalobis Distance adalah perbedaan ke Euclidean dan Manhattan distance yaitu jarak ini terbebas dari hubungan data yang mana dua data berasal $$ d_{mah}=\\sqrt{(x-y)S {-1}(x-y) {T}} $$","title":"Mahalanobis Distance"},{"location":"jarak-data/#cosine-measure","text":"Persammaan Cosine kebanyakan digunakan untuk mencari kemiripan dokumen dan rumusnya adalah: $$ Cosine(x,y)=\\frac{\\sum_{i=1}^{n}x_{i}{y_{i}}}{||x||2||y||2} $$","title":"Cosine Measure"},{"location":"jarak-data/#pearson-correlation","text":"Pearson Correlation Digunakan dalam clustering ekspresi data. Ukuran kemiripan menghitung kemiripan antara bentuk dari dua pola ekspresi. Rumusnya adalah: $$ Pearson(x,y)=\\frac{\\sum_{i=1} n(x_{i}-\\mu_{x})(y_{i}-\\mu_{y})}{\\sqrt{\\sum_{i=1} n(x_{i}-y_{i}) 2}\\sqrt{\\sum_{i=1} n(x_{i}-y_{i})^2}} $$ Dimana mu x dan mu y dimaksudkan untuk x dan y. Jarak ini juga memiliki kelemahan yaitu sensitif terhadap outliers Contoh Program menghitung jarak data dari data di internet: import pandas as pd df = pd . read_csv ( \"tae.csv\" , nrows = 4 ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } English Instructor Course Summer or Regular Class Size Class Attribut 0 1 23 3 1 19 3 1 2 15 3 1 17 3 2 1 23 3 2 49 3 3 1 5 2 2 33 3 binary = [ 0 , 3 ] num = [ 1 , 2 , 4 , 5 ] from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ 0 ] + [ 0 ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Binary v1-v2 0 0 0 v1-v3 0 0 0 v2-v3 0 0 0 v3-v4 0 0 0 def chordDist ( v1 , v2 , jnis ): jmlh = 0 normv1 = 0 normv2 = 0 for x in range ( len ( jnis )): normv1 = normv1 + ( int ( df . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) normv2 = normv2 + ( int ( df . values . tolist ()[ v2 ][ jnis [ x ]]) ** 2 ) jmlh = jmlh + ( int ( df . values . tolist ()[ v1 ][ jnis [ x ]]) * int ( df . values . tolist ()[ v2 ][ jnis [ x ]])) return (( 2 - ( 2 * jmlh / ( normv1 * normv2 ))) ** 0.5 ) from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ chordDist ( 0 , 1 , num )] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ chordDist ( 0 , 2 , num )] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ chordDist ( 1 , 2 , num )] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ chordDist ( 2 , 3 , num )] + [ 0 ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Binary v1-v2 0 1.4132090254432463 0 v1-v3 0 1.4138230758312995 0 v2-v3 0 1.4136742257751354 0 v3-v4 0 1.413841698820558 0 BINARY def binaryDist ( v1 , v2 , jnis ): q = 0 r = 0 s = 0 t = 0 for x in range ( len ( jnis )): if ( int ( df . values . tolist ()[ v1 ][ jnis [ x ]])) == 1 and ( int ( df . values . tolist ()[ v2 ][ jnis [ x ]])) == 1 : q = q + 1 elif ( int ( df . values . tolist ()[ v1 ][ jnis [ x ]])) == 1 and ( int ( df . values . tolist ()[ v2 ][ jnis [ x ]])) == 2 : r = r + 1 elif ( int ( df . values . tolist ()[ v1 ][ jnis [ x ]])) == 2 and ( int ( df . values . tolist ()[ v2 ][ jnis [ x ]])) == 1 : s = s + 1 else : t = t + 1 return (( r + s ) / ( q + r + s + t )) from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ chordDist ( 0 , 1 , num )] + [ binaryDist ( 0 , 1 , binary )], [ \"v1-v3\" ] + [ 0 ] + [ chordDist ( 0 , 2 , num )] + [ binaryDist ( 0 , 2 , binary )], [ \"v2-v3\" ] + [ 0 ] + [ chordDist ( 1 , 2 , num )] + [ binaryDist ( 1 , 2 , binary )], [ \"v3-v4\" ] + [ 0 ] + [ chordDist ( 2 , 3 , num )] + [ binaryDist ( 2 , 3 , binary )], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Binary v1-v2 0 1.4132090254432463 0.5 v1-v3 0 1.4138230758312995 0.5 v2-v3 0 1.4136742257751354 1.0 v3-v4 0 1.413841698820558 0.0","title":"Pearson Correlation"},{"location":"jarak-data/#referensi","text":"https://www.researchgate.net/publication/286637899_A_Comparison_Study_on_Similarity_and_Dissimilarity_Measures_in_Clustering_Continuous_Data mulaab.github.io/datamining/memahami-data/","title":"Referensi"},{"location":"knn-weighted/","text":"KKN/K-Nearest Neighbor Classifier adalah metode pengelompokan data dengan cara membandingkan data baru dengan K data terdekat, metode ini juga disebut juga dengan Lazy Algorithm karena cara perhitungannya yang mudah serta tingkat keakurasian yang tinggi Mengklasifikasi data menggunakan metode weighted KNN. Menggunakan data iris yang diambil acak sebanyak 100 data. dengan data sampel sebagai berikut : sepal length sepal width petal length petal width variety 5.2 3.3 1.5 0.5 Setosa Data training : a b c d class 5.8 2.6 4 1.2 Versicolor 5.5 2.5 4 1.3 Versicolor 5.6 3 4.1 1.3 Versicolor 5.8 2.7 4.1 1 Versicolor 5.7 2.8 4.1 1.3 Versicolor 6.1 2.8 4 1.3 Versicolor 5.5 2.3 4 1.3 Versicolor 5.7 3 4.2 1.2 Versicolor 5.7 2.9 4.2 1.3 Versicolor 6 2.2 4 1 Versicolor 5.6 2.7 4.2 1.3 Versicolor 5.9 3 4.2 1.5 Versicolor 5.5 2.6 4.4 1.2 Versicolor 6.2 2.9 4.3 1.3 Versicolor 6.4 2.9 4.3 1.3 Versicolor 5.4 3 4.5 1.5 Versicolor 5.7 2.8 4.5 1.3 Versicolor 6.4 3.2 5.3 2.3 Virginica 6.4 3.1 5.5 1.8 Virginica 6.7 3 5.2 2.3 Virginica 6.2 3.4 5.4 2.3 Virginica 6.5 3 5.5 1.8 Virginica 6.3 2.9 5.6 1.8 Virginica 6.9 3.1 5.4 2.1 Virginica 6.4 2.8 5.6 2.1 Virginica 6.8 3 5.5 2.1 Virginica 6.4 2.8 5.6 2.2 Virginica 6.3 3.4 5.6 2.4 Virginica 6.7 3.3 5.7 2.1 Virginica 6.7 3.1 5.6 2.4 Virginica 6.7 2.5 5.8 1.8 Virginica 6.5 3 5.8 2.2 Virginica 6.9 3.2 5.7 2.3 Virginica 7.2 3 5.8 1.6 Virginica 6.7 3.3 5.7 2.5 Virginica 5.1 3.3 1.7 0.5 Setosa 5.4 3.4 1.5 0.4 Setosa 5 3.4 1.6 0.4 Setosa 5 3.5 1.6 0.6 Setosa 5.1 3.5 1.4 0.3 Setosa 5.2 3.4 1.4 0.2 Setosa 5.1 3.4 1.5 0.2 Setosa 5.2 3.5 1.5 0.2 Setosa 5 3.4 1.5 0.2 Setosa 5.1 3.5 1.4 0.2 Setosa 5 3.5 1.3 0.3 Setosa 5.4 3.4 1.7 0.2 Setosa 5.1 3.7 1.5 0.4 Setosa 5 3.2 1.2 0.2 Setosa 5 3.6 1.4 0.2 Setosa 4.3 3 1.1 0.1 Setosa 4.3 3 1.1 0.1 Setosa 5 2.3 3.3 1 Versicolor 5.7 2.6 3.5 1 Versicolor 5.6 2.9 3.6 1.3 Versicolor 5 2 3.5 1 Versicolor 5.5 2.4 3.7 1 Versicolor 5.5 2.4 3.8 1.1 Versicolor 5.6 2.5 3.9 1.1 Versicolor 5.2 2.7 3.9 1.4 Versicolor 5.8 2.7 3.9 1.2 Versicolor 5.3 3.7 1.5 0.2 Setosa 5.1 3.8 1.5 0.3 Setosa 4.9 3.6 1.4 0.1 Setosa 4.7 3.2 1.6 0.2 Setosa 5.1 3.8 1.6 0.2 Setosa 4.7 3.2 1.3 0.2 Setosa 4.8 3.4 1.9 0.2 Setosa 4.6 3.4 1.4 0.3 Setosa 4.6 3.2 1.4 0.2 Setosa 4.6 3.1 1.5 0.2 Setosa 4.4 3.2 1.3 0.2 Setosa 4.4 3 1.3 0.2 Setosa 4.4 2.9 1.4 0.2 Setosa 5.6 3 4.5 1.5 Versicolor 6 3.4 4.5 1.6 Versicolor 6.6 3 4.4 1.4 Versicolor 6.1 3 4.6 1.4 Versicolor 6.2 2.8 4.8 1.8 Virginica 6.1 3 4.9 1.8 Virginica 5.6 2.8 4.9 2 Virginica 6.5 3.2 5.1 2 Virginica 5.8 2.8 5.1 2.4 Virginica 6.5 3 5.2 2 Virginica 6.4 2.7 5.3 1.9 Virginica 6.1 2.6 5.6 1.4 Virginica 6.9 3.1 5.1 2.3 Virginica 6.8 3.2 5.9 2.3 Virginica 6.3 3.3 6 2.5 Virginica 7.1 3 5.9 2.1 Virginica 7.2 3.2 6 1.8 Virginica 7.4 2.8 6.1 1.9 Virginica 7.2 3.6 6.1 2.5 Virginica 7.3 2.9 6.3 1.8 Virginica 7.7 3 6.1 2.3 Virginica 7.9 3.8 6.4 2 Virginica 7.6 3 6.6 2.1 Virginica 7.7 2.8 6.7 2 Virginica 7.7 3.8 6.7 2.2 Virginica 7.7 2.6 6.9 2.3 Virginica Menghitung Jarak a b c d class (a-sd1)^2 (b-sd2)^2 (c-sd3)^2 (d-sd4)^2 SQRT 5.1 3.3 1.7 0.5 Setosa 0.01 0 0.04 0 0.223607 5.4 3.4 1.5 0.4 Setosa 0.04 0.01 0 0.01 0.244949 5 3.4 1.6 0.4 Setosa 0.04 0.01 0.01 0.01 0.264575 5 3.5 1.6 0.6 Setosa 0.04 0.04 0.01 0.01 0.316228 5.1 3.5 1.4 0.3 Setosa 0.01 0.04 0.01 0.04 0.316228 5.2 3.4 1.4 0.2 Setosa 0 0.01 0.01 0.09 0.331662 5.1 3.4 1.5 0.2 Setosa 0.01 0.01 0 0.09 0.331662 5.2 3.5 1.5 0.2 Setosa 0 0.04 0 0.09 0.360555 5 3.4 1.5 0.2 Setosa 0.04 0.01 0 0.09 0.374166 5.1 3.5 1.4 0.2 Setosa 0.01 0.04 0.01 0.09 0.387298 5 3.5 1.3 0.3 Setosa 0.04 0.04 0.04 0.04 0.4 5.4 3.4 1.7 0.2 Setosa 0.04 0.01 0.04 0.09 0.424264 5.1 3.7 1.5 0.4 Setosa 0.01 0.16 0 0.01 0.424264 5 3.2 1.2 0.2 Setosa 0.04 0.01 0.09 0.09 0.479583 5 3.6 1.4 0.2 Setosa 0.04 0.09 0.01 0.09 0.479583 5.3 3.7 1.5 0.2 Setosa 0.01 0.16 0 0.09 0.509902 5.1 3.8 1.5 0.3 Setosa 0.01 0.25 0 0.04 0.547723 4.9 3.6 1.4 0.1 Setosa 0.09 0.09 0.01 0.16 0.591608 4.7 3.2 1.6 0.2 Setosa 0.25 0.01 0.01 0.09 0.6 5.1 3.8 1.6 0.2 Setosa 0.01 0.25 0.01 0.09 0.6 4.7 3.2 1.3 0.2 Setosa 0.25 0.01 0.04 0.09 0.6245 4.8 3.4 1.9 0.2 Setosa 0.16 0.01 0.16 0.09 0.648074 4.6 3.4 1.4 0.3 Setosa 0.36 0.01 0.01 0.04 0.648074 4.6 3.2 1.4 0.2 Setosa 0.36 0.01 0.01 0.09 0.685565 4.6 3.1 1.5 0.2 Setosa 0.36 0.04 0 0.09 0.7 4.4 3.2 1.3 0.2 Setosa 0.64 0.01 0.04 0.09 0.883176 4.4 3 1.3 0.2 Setosa 0.64 0.09 0.04 0.09 0.927362 4.4 2.9 1.4 0.2 Setosa 0.64 0.16 0.01 0.09 0.948683 4.3 3 1.1 0.1 Setosa 0.81 0.09 0.16 0.16 1.104536 4.3 3 1.1 0.1 Setosa 0.81 0.09 0.16 0.16 1.104536 5 2.3 3.3 1 Versicolor 0.04 1 3.24 0.25 2.12838 5.7 2.6 3.5 1 Versicolor 0.25 0.49 4 0.25 2.233831 5.6 2.9 3.6 1.3 Versicolor 0.16 0.16 4.41 0.64 2.317326 5 2 3.5 1 Versicolor 0.04 1.69 4 0.25 2.445404 5.5 2.4 3.7 1 Versicolor 0.09 0.81 4.84 0.25 2.447448 5.5 2.4 3.8 1.1 Versicolor 0.09 0.81 5.29 0.36 2.559297 5.6 2.5 3.9 1.1 Versicolor 0.16 0.64 5.76 0.36 2.630589 5.2 2.7 3.9 1.4 Versicolor 0 0.36 5.76 0.81 2.632489 5.8 2.7 3.9 1.2 Versicolor 0.36 0.36 5.76 0.49 2.640076 5.8 2.6 4 1.2 Versicolor 0.36 0.49 6.25 0.49 2.754995 5.5 2.5 4 1.3 Versicolor 0.09 0.64 6.25 0.64 2.760435 5.6 3 4.1 1.3 Versicolor 0.16 0.09 6.76 0.64 2.765863 5.8 2.7 4.1 1 Versicolor 0.36 0.36 6.76 0.25 2.780288 5.7 2.8 4.1 1.3 Versicolor 0.25 0.25 6.76 0.64 2.810694 6.1 2.8 4 1.3 Versicolor 0.81 0.25 6.25 0.64 2.819574 5.5 2.3 4 1.3 Versicolor 0.09 1 6.25 0.64 2.824889 5.7 3 4.2 1.2 Versicolor 0.25 0.09 7.29 0.49 2.849561 5.7 2.9 4.2 1.3 Versicolor 0.25 0.16 7.29 0.64 2.887906 6 2.2 4 1 Versicolor 0.64 1.21 6.25 0.25 2.889637 5.6 2.7 4.2 1.3 Versicolor 0.16 0.36 7.29 0.64 2.906888 5.9 3 4.2 1.5 Versicolor 0.49 0.09 7.29 1 2.978255 5.5 2.6 4.4 1.2 Versicolor 0.09 0.49 8.41 0.49 3.078961 6.2 2.9 4.3 1.3 Versicolor 1 0.16 7.84 0.64 3.104835 6.4 2.9 4.3 1.3 Versicolor 1.44 0.16 7.84 0.64 3.174902 5.4 3 4.5 1.5 Versicolor 0.04 0.09 9 1 3.182766 5.7 2.8 4.5 1.3 Versicolor 0.25 0.25 9 0.64 3.184337 5.6 3 4.5 1.5 Versicolor 0.16 0.09 9 1 3.201562 6 3.4 4.5 1.6 Versicolor 0.64 0.01 9 1.21 3.295451 6.6 3 4.4 1.4 Versicolor 1.96 0.09 8.41 0.81 3.357082 6.1 3 4.6 1.4 Versicolor 0.81 0.09 9.61 0.81 3.364521 6.2 2.8 4.8 1.8 Virginica 1 0.25 10.89 1.69 3.718871 6.1 3 4.9 1.8 Virginica 0.81 0.09 11.56 1.69 3.761649 5.6 2.8 4.9 2 Virginica 0.16 0.25 11.56 2.25 3.770942 6.5 3.2 5.1 2 Virginica 1.69 0.01 12.96 2.25 4.112177 5.8 2.8 5.1 2.4 Virginica 0.36 0.25 12.96 3.61 4.144876 6.5 3 5.2 2 Virginica 1.69 0.09 13.69 2.25 4.209513 6.4 2.7 5.3 1.9 Virginica 1.44 0.36 14.44 1.96 4.266146 6.1 2.6 5.6 1.4 Virginica 0.81 0.49 16.81 0.81 4.349713 6.9 3.1 5.1 2.3 Virginica 2.89 0.04 12.96 3.24 4.373786 6.4 3.2 5.3 2.3 Virginica 1.44 0.01 14.44 3.24 4.373786 6.4 3.1 5.5 1.8 Virginica 1.44 0.04 16 1.69 4.378356 6.7 3 5.2 2.3 Virginica 2.25 0.09 13.69 3.24 4.389761 6.2 3.4 5.4 2.3 Virginica 1 0.01 15.21 3.24 4.411349 6.5 3 5.5 1.8 Virginica 1.69 0.09 16 1.69 4.412482 6.3 2.9 5.6 1.8 Virginica 1.21 0.16 16.81 1.69 4.457578 6.9 3.1 5.4 2.1 Virginica 2.89 0.04 15.21 2.56 4.549725 6.4 2.8 5.6 2.1 Virginica 1.44 0.25 16.81 2.56 4.589118 6.8 3 5.5 2.1 Virginica 2.56 0.09 16 2.56 4.605432 6.4 2.8 5.6 2.2 Virginica 1.44 0.25 16.81 2.89 4.624932 6.3 3.4 5.6 2.4 Virginica 1.21 0.01 16.81 3.61 4.651881 6.7 3.3 5.7 2.1 Virginica 2.25 0 17.64 2.56 4.738143 6.7 3.1 5.6 2.4 Virginica 2.25 0.04 16.81 3.61 4.765501 6.7 2.5 5.8 1.8 Virginica 2.25 0.64 18.49 1.69 4.803124 6.5 3 5.8 2.2 Virginica 1.69 0.09 18.49 2.89 4.812484 6.9 3.2 5.7 2.3 Virginica 2.89 0.01 17.64 3.24 4.876474 7.2 3 5.8 1.6 Virginica 4 0.09 18.49 1.21 4.877499 6.7 3.3 5.7 2.5 Virginica 2.25 0 17.64 4 4.88774 6.8 3.2 5.9 2.3 Virginica 2.56 0.01 19.36 3.24 5.016971 6.3 3.3 6 2.5 Virginica 1.21 0 20.25 4 5.04579 7.1 3 5.9 2.1 Virginica 3.61 0.09 19.36 2.56 5.06162 7.2 3.2 6 1.8 Virginica 4 0.01 20.25 1.69 5.094114 7.4 2.8 6.1 1.9 Virginica 4.84 0.25 21.16 1.96 5.311309 7.2 3.6 6.1 2.5 Virginica 4 0.09 21.16 4 5.408327 7.3 2.9 6.3 1.8 Virginica 4.41 0.16 23.04 1.69 5.412947 7.7 3 6.1 2.3 Virginica 6.25 0.09 21.16 3.24 5.544367 7.9 3.8 6.4 2 Virginica 7.29 0.25 24.01 2.25 5.813777 7.6 3 6.6 2.1 Virginica 5.76 0.09 26.01 2.56 5.866856 7.7 2.8 6.7 2 Virginica 6.25 0.25 27.04 2.25 5.982474 7.7 3.8 6.7 2.2 Virginica 6.25 0.25 27.04 2.89 6.035727 7.7 2.6 6.9 2.3 Virginica 6.25 0.49 29.16 3.24 6.256197 K=5 e 1/e setosa versicolor virginica 0.223607 4.472132 4.472132 0 0 0.244949 4.082482 4.082482 0 0 0.264575 3.779647 3.779647 0 0 0.316228 3.162275 3.162275 0 0 0.331662 3.015118 3.015118 0 0 CLASS : SETOSA","title":"KKN Weighted"},{"location":"mclaurin-series/","text":"Deret Mclaurin sangat berguna dalam komputasi numerik dalam menghitung nilai - nilai fungsi yang susah dihitung secara manual. Contoh sin(x), cos(x), e^x Didalam kasus kali ini jika masalahnya adalah: $$ f(x)=e^{2x} $$ maka turunannya adalah: $$ \\frac{f 1(x)}{n!}=\\frac{2e {2x}}{1!} $$ \\frac{f^2(x)}{n!}=\\frac{4e^{2x}}{2!} \\frac{f^2(x)}{n!}=\\frac{4e^{2x}}{2!} \\frac{f^3(x)}{n!}=\\frac{4e^{2x}}{3!} \\frac{f^3(x)}{n!}=\\frac{4e^{2x}}{3!} \\frac{f^4(x)}{n!}=\\frac{8e^{2x}}{4!}\\\\ \\vdots \\frac{f^4(x)}{n!}=\\frac{8e^{2x}}{4!}\\\\ \\vdots maka deret ekspansinya adalah: $$ \\displaystyle\\sum_{n=0} {\\infty}\\frac{2 nx^n}{n!} $$ Listing Program \u00b6 Lakukan ekspansi dari kasus diatas sampai error lebih kecil daripada 0,0001 def faktorial ( n ): if n == 0 : return 1 else : return n * faktorial ( n - 1 ) k = 1 x = 4 counter = 0 putaran = 1 def mclaurin ( k , x , counter , hasil , putaran ): end = False while end == False : k = k * 2 print ( k , '(e^(2x)' ) hasil += ( k * ( x ** counter )) / faktorial ( counter ) residu = hasil + ( k * 2 ) * ( x ** counter + 1 ) / faktorial ( counter + 1 ) error = residu - hasil print ( 'error putaran ke' , putaran , ' adalah' , error ) if error <= 0.001 : print ( 'error adalah =' , error ) print ( \"berhenti pada putaran ke =\" , putaran ) break else : counter += 1 putaran += 1 print ( 'Menghitung Deret McLaurin 2e^(2x) \\n Sampai error lebih kecil daripada 0,0001===========================================' ) mclaurin ( k , x , counter , 0 , 1 ) Menghitung Deret McLaurin 2e^(2x) Sampai error lebih kecil daripada 0,0001=========================================== 2 (e^(2x) error putaran ke 1 adalah 8.0 4 (e^(2x) error putaran ke 2 adalah 20.0 8 (e^(2x) error putaran ke 3 adalah 45.33333333333334 16 (e^(2x) error putaran ke 4 adalah 86.66666666666666 32 (e^(2x) error putaran ke 5 adalah 137.0666666666666 64 (e^(2x) error putaran ke 6 adalah 182.22222222222217 128 (e^(2x) error putaran ke 7 adalah 208.1015873015872 256 (e^(2x) error putaran ke 8 adalah 208.06349206349205 512 (e^(2x) error putaran ke 9 adalah 184.93686067019416 1024 (e^(2x) error putaran ke 10 adalah 147.94779541446223 2048 (e^(2x) error putaran ke 11 adalah 107.5980888247559 4096 (e^(2x) error putaran ke 12 adalah 71.73200790978535 8192 (e^(2x) error putaran ke 13 adalah 44.14276620498822 16384 (e^(2x) error putaran ke 14 adalah 25.224436703801985 32768 (e^(2x) error putaran ke 15 adalah 13.453032758345216 65536 (e^(2x) error putaran ke 16 adalah 6.72651636037881 131072 (e^(2x) error putaran ke 17 adalah 3.16541946149664 262144 (e^(2x) error putaran ke 18 adalah 1.406853093752943 524288 (e^(2x) error putaran ke 19 adalah 0.5923591973441944 1048576 (e^(2x) error putaran ke 20 adalah 0.23694367893494928 2097152 (e^(2x) error putaran ke 21 adalah 0.09026425864158227 4194304 (e^(2x) error putaran ke 22 adalah 0.03282336677875719 8388608 (e^(2x) error putaran ke 23 adalah 0.011416823227591522 16777216 (e^(2x) error putaran ke 24 adalah 0.003805607742833672 33554432 (e^(2x) error putaran ke 25 adalah 0.0012177944772702176 67108864 (e^(2x) error putaran ke 26 adalah 0.0003747059927263763 error adalah = 0.0003747059927263763 berhenti pada putaran ke = 26","title":"Mclaurin Series"},{"location":"mclaurin-series/#listing-program","text":"Lakukan ekspansi dari kasus diatas sampai error lebih kecil daripada 0,0001 def faktorial ( n ): if n == 0 : return 1 else : return n * faktorial ( n - 1 ) k = 1 x = 4 counter = 0 putaran = 1 def mclaurin ( k , x , counter , hasil , putaran ): end = False while end == False : k = k * 2 print ( k , '(e^(2x)' ) hasil += ( k * ( x ** counter )) / faktorial ( counter ) residu = hasil + ( k * 2 ) * ( x ** counter + 1 ) / faktorial ( counter + 1 ) error = residu - hasil print ( 'error putaran ke' , putaran , ' adalah' , error ) if error <= 0.001 : print ( 'error adalah =' , error ) print ( \"berhenti pada putaran ke =\" , putaran ) break else : counter += 1 putaran += 1 print ( 'Menghitung Deret McLaurin 2e^(2x) \\n Sampai error lebih kecil daripada 0,0001===========================================' ) mclaurin ( k , x , counter , 0 , 1 ) Menghitung Deret McLaurin 2e^(2x) Sampai error lebih kecil daripada 0,0001=========================================== 2 (e^(2x) error putaran ke 1 adalah 8.0 4 (e^(2x) error putaran ke 2 adalah 20.0 8 (e^(2x) error putaran ke 3 adalah 45.33333333333334 16 (e^(2x) error putaran ke 4 adalah 86.66666666666666 32 (e^(2x) error putaran ke 5 adalah 137.0666666666666 64 (e^(2x) error putaran ke 6 adalah 182.22222222222217 128 (e^(2x) error putaran ke 7 adalah 208.1015873015872 256 (e^(2x) error putaran ke 8 adalah 208.06349206349205 512 (e^(2x) error putaran ke 9 adalah 184.93686067019416 1024 (e^(2x) error putaran ke 10 adalah 147.94779541446223 2048 (e^(2x) error putaran ke 11 adalah 107.5980888247559 4096 (e^(2x) error putaran ke 12 adalah 71.73200790978535 8192 (e^(2x) error putaran ke 13 adalah 44.14276620498822 16384 (e^(2x) error putaran ke 14 adalah 25.224436703801985 32768 (e^(2x) error putaran ke 15 adalah 13.453032758345216 65536 (e^(2x) error putaran ke 16 adalah 6.72651636037881 131072 (e^(2x) error putaran ke 17 adalah 3.16541946149664 262144 (e^(2x) error putaran ke 18 adalah 1.406853093752943 524288 (e^(2x) error putaran ke 19 adalah 0.5923591973441944 1048576 (e^(2x) error putaran ke 20 adalah 0.23694367893494928 2097152 (e^(2x) error putaran ke 21 adalah 0.09026425864158227 4194304 (e^(2x) error putaran ke 22 adalah 0.03282336677875719 8388608 (e^(2x) error putaran ke 23 adalah 0.011416823227591522 16777216 (e^(2x) error putaran ke 24 adalah 0.003805607742833672 33554432 (e^(2x) error putaran ke 25 adalah 0.0012177944772702176 67108864 (e^(2x) error putaran ke 26 adalah 0.0003747059927263763 error adalah = 0.0003747059927263763 berhenti pada putaran ke = 26","title":"Listing Program"},{"location":"memahami-data/","text":"Memahami Data \u00b6 Ada banyak jenis data dan masing masing jenisnya membutuhkan tools dan teknik yang berbeda beda Data terbagi menjadi beberapa jenis yaitu Data Terstruktur Data tidak terstruktur Data bahasa alami Machine Generated Data Data audio visual Data Streaming Data berbentuk Graph Type Data Atribut \u00b6 Atribut Biner \u00b6 termasuk nominal karena hanya kategori 0 dan 1 \u200b ada 2 bentuk : Atribut simetris \u00b6 jika keduanya memiliki nilai yang sama (contoh: Tes Urin, Tes Buta Warna (Positif dan Negatif)) \u200b Atribut Asimetris jika hasil dari nilai tidak sama pentingnya (contoh: hasil tes medis) Atribut Ordinal \u00b6 adalah atribut dengan nilai yang memliki arti urutan atau peringkat tapi besarnya nilai berurutan tersebut tidak diketahui (Contoh: Sangat puas, cukup puas, tidak puas) \u200b Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median (nilai tengah) Atribut Numerik \u00b6 bersifat kuantitatif terukur dinyatakan dengan bilangan bulat atau nilai riel Program untuk menampilkan statistik deskriptif dari kumpulan data: from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd . read_csv ( 'Tinggi.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Tinggi (cm) Berat Badan (kg) Panjang Kaki (size) Umur (tahun) 0 178 109 45 18 1 163 84 42 20 2 184 79 40 20 3 163 82 42 21 4 168 122 42 18 5 178 83 39 20 6 167 108 43 18 7 177 119 44 18 8 175 71 43 19 9 181 114 37 21 10 183 93 38 18 11 171 71 45 20 12 178 116 42 20 13 176 107 41 21 14 169 88 41 20 15 160 101 37 22 16 172 76 43 20 17 181 87 39 19 18 164 75 44 22 19 171 115 41 20 20 181 107 44 18 21 176 100 45 18 22 167 81 37 19 23 163 116 43 21 24 176 96 45 20 25 179 114 36 20 26 180 72 45 22 27 176 78 42 21 28 178 86 44 20 29 168 107 45 21 ... ... ... ... ... 70 179 121 45 21 71 166 96 43 19 72 161 88 43 21 73 176 98 36 19 74 176 115 38 21 75 178 114 38 19 76 178 124 41 21 77 173 103 36 18 78 180 124 36 20 79 167 100 40 20 80 168 97 39 18 81 160 108 36 18 82 170 119 43 18 83 162 86 37 20 84 178 95 45 18 85 171 104 44 21 86 170 96 38 18 87 185 98 37 20 88 163 93 39 18 89 178 86 39 19 90 168 71 36 20 91 166 114 38 21 92 179 76 43 20 93 171 124 43 18 94 178 71 42 22 95 174 104 43 21 96 162 92 41 22 97 165 89 39 20 98 163 82 42 22 99 170 88 42 19 100 rows \u00d7 4 columns from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"describe()\" ] + [ '<pre>' + str ( df [ col ] . describe ()) + '</pre>' for col in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \"{:.2f}\" . format ( df [ col ] . std ()) for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3()\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew()\" ] + [ \"{:.2f}\" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) method Tinggi (cm) Berat Badan (kg) Panjang Kaki (size) Umur (tahun) describe() count 100.000000 mean 172.040000 std 7.315295 min 160.000000 25% 165.000000 50% 172.000000 75% 178.000000 max 185.000000 Name: Tinggi (cm), dtype: float64 count 100.000000 mean 96.960000 std 15.966455 min 71.000000 25% 83.750000 50% 97.000000 75% 109.000000 max 124.000000 Name: Berat Badan (kg), dtype: float64 count 100.000000 mean 40.790000 std 2.857897 min 36.000000 25% 38.000000 50% 41.000000 75% 43.000000 max 45.000000 Name: Panjang Kaki (size), dtype: float64 count 100.000000 mean 19.860000 std 1.325888 min 18.000000 25% 19.000000 50% 20.000000 75% 21.000000 max 22.000000 Name: Umur (tahun), dtype: float64 count() 100 100 100 100 mean() 172.04 96.96 40.79 19.86 std() 7.32 15.97 2.86 1.33 min() 160 71 36 18 max() 185 124 45 22 q1() 165.0 83.75 38.0 19.0 q2() 172.0 97.0 41.0 20.0 q3() 178.0 109.0 43.0 21.0 skew() -0.02 -0.03 -0.25 -0.11 Mean \u00b6 Mean atau yang bisa disebut rata rata adalah jumlah data dibagi dengan banyaknya data Modus \u00b6 Data yang paling banyak keluar/menonjol dari kumpulan data Median \u00b6 Nilai tengah dari banyaknya data Quartil \u00b6 Membagi data menjadi sama banyak yang dibatasi oleh suatu nilai Kuartil bawah ( Q1 ) Kuartil tengah ( Q2 ) Kuartil atas ( Q3 ) Skewness \u00b6 Suatu ketidakseimbangan dan asimetris mean dari distribusi data, jika data itu normal maka saat didistribusikan menggunakan bell curve,maka kurvanya akan simetris","title":"Memahami data"},{"location":"memahami-data/#memahami-data","text":"Ada banyak jenis data dan masing masing jenisnya membutuhkan tools dan teknik yang berbeda beda Data terbagi menjadi beberapa jenis yaitu Data Terstruktur Data tidak terstruktur Data bahasa alami Machine Generated Data Data audio visual Data Streaming Data berbentuk Graph","title":"Memahami Data"},{"location":"memahami-data/#type-data-atribut","text":"","title":"Type Data Atribut"},{"location":"memahami-data/#atribut-biner","text":"termasuk nominal karena hanya kategori 0 dan 1 \u200b ada 2 bentuk :","title":"Atribut Biner"},{"location":"memahami-data/#atribut-simetris","text":"jika keduanya memiliki nilai yang sama (contoh: Tes Urin, Tes Buta Warna (Positif dan Negatif)) \u200b Atribut Asimetris jika hasil dari nilai tidak sama pentingnya (contoh: hasil tes medis)","title":"Atribut simetris"},{"location":"memahami-data/#atribut-ordinal","text":"adalah atribut dengan nilai yang memliki arti urutan atau peringkat tapi besarnya nilai berurutan tersebut tidak diketahui (Contoh: Sangat puas, cukup puas, tidak puas) \u200b Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median (nilai tengah)","title":"Atribut Ordinal"},{"location":"memahami-data/#atribut-numerik","text":"bersifat kuantitatif terukur dinyatakan dengan bilangan bulat atau nilai riel Program untuk menampilkan statistik deskriptif dari kumpulan data: from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd . read_csv ( 'Tinggi.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Tinggi (cm) Berat Badan (kg) Panjang Kaki (size) Umur (tahun) 0 178 109 45 18 1 163 84 42 20 2 184 79 40 20 3 163 82 42 21 4 168 122 42 18 5 178 83 39 20 6 167 108 43 18 7 177 119 44 18 8 175 71 43 19 9 181 114 37 21 10 183 93 38 18 11 171 71 45 20 12 178 116 42 20 13 176 107 41 21 14 169 88 41 20 15 160 101 37 22 16 172 76 43 20 17 181 87 39 19 18 164 75 44 22 19 171 115 41 20 20 181 107 44 18 21 176 100 45 18 22 167 81 37 19 23 163 116 43 21 24 176 96 45 20 25 179 114 36 20 26 180 72 45 22 27 176 78 42 21 28 178 86 44 20 29 168 107 45 21 ... ... ... ... ... 70 179 121 45 21 71 166 96 43 19 72 161 88 43 21 73 176 98 36 19 74 176 115 38 21 75 178 114 38 19 76 178 124 41 21 77 173 103 36 18 78 180 124 36 20 79 167 100 40 20 80 168 97 39 18 81 160 108 36 18 82 170 119 43 18 83 162 86 37 20 84 178 95 45 18 85 171 104 44 21 86 170 96 38 18 87 185 98 37 20 88 163 93 39 18 89 178 86 39 19 90 168 71 36 20 91 166 114 38 21 92 179 76 43 20 93 171 124 43 18 94 178 71 42 22 95 174 104 43 21 96 162 92 41 22 97 165 89 39 20 98 163 82 42 22 99 170 88 42 19 100 rows \u00d7 4 columns from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"describe()\" ] + [ '<pre>' + str ( df [ col ] . describe ()) + '</pre>' for col in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \"{:.2f}\" . format ( df [ col ] . std ()) for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3()\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew()\" ] + [ \"{:.2f}\" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) method Tinggi (cm) Berat Badan (kg) Panjang Kaki (size) Umur (tahun) describe() count 100.000000 mean 172.040000 std 7.315295 min 160.000000 25% 165.000000 50% 172.000000 75% 178.000000 max 185.000000 Name: Tinggi (cm), dtype: float64 count 100.000000 mean 96.960000 std 15.966455 min 71.000000 25% 83.750000 50% 97.000000 75% 109.000000 max 124.000000 Name: Berat Badan (kg), dtype: float64 count 100.000000 mean 40.790000 std 2.857897 min 36.000000 25% 38.000000 50% 41.000000 75% 43.000000 max 45.000000 Name: Panjang Kaki (size), dtype: float64 count 100.000000 mean 19.860000 std 1.325888 min 18.000000 25% 19.000000 50% 20.000000 75% 21.000000 max 22.000000 Name: Umur (tahun), dtype: float64 count() 100 100 100 100 mean() 172.04 96.96 40.79 19.86 std() 7.32 15.97 2.86 1.33 min() 160 71 36 18 max() 185 124 45 22 q1() 165.0 83.75 38.0 19.0 q2() 172.0 97.0 41.0 20.0 q3() 178.0 109.0 43.0 21.0 skew() -0.02 -0.03 -0.25 -0.11","title":"Atribut Numerik"},{"location":"memahami-data/#mean","text":"Mean atau yang bisa disebut rata rata adalah jumlah data dibagi dengan banyaknya data","title":"Mean"},{"location":"memahami-data/#modus","text":"Data yang paling banyak keluar/menonjol dari kumpulan data","title":"Modus"},{"location":"memahami-data/#median","text":"Nilai tengah dari banyaknya data","title":"Median"},{"location":"memahami-data/#quartil","text":"Membagi data menjadi sama banyak yang dibatasi oleh suatu nilai Kuartil bawah ( Q1 ) Kuartil tengah ( Q2 ) Kuartil atas ( Q3 )","title":"Quartil"},{"location":"memahami-data/#skewness","text":"Suatu ketidakseimbangan dan asimetris mean dari distribusi data, jika data itu normal maka saat didistribusikan menggunakan bell curve,maka kurvanya akan simetris","title":"Skewness"},{"location":"newton-raphson/","text":"Newton Raphson \u00b6 Newton-Raphson \u00b6 Dalam analisis numerik, metode Newton / Newton-Raphson yang mendapat nama dari Isaac Newton dan Joseph Rapshon, merupakan metode yang paling dikenal untuk mencari akar suatu fungsi f(x) dengan pendekatan satu titik dimana fungsi f(x) mempunyai turunan. Metode Newton-Raphson dapat diturunkan berdasarkan interpretasi geometrik (sebuah metode alternatif yang didasarkan pada Deret Taylor ). Prosedur Metode Newton : menentukan x_0 sebagai titik awal, kemudian menarik garis lurus yang menyinggung titik f(x_0) . Hal ini berakibat garis I memotong sumbu x di titik x_1 Setelah itu diulangi langkah sebelumnya tapi sekarang x_1 dianggap sebagai titik awalnya. Dari mengulang langkah-langkah sebelumnya akan mendapatkan x_2 , x_3 , ... , x_n dengan x_n yang diperoleh adalah bilangan riil yang merupakan akar atau mendekati akar yang sebenarnya. persamaan garis I : y - y_0 = m(x - x_0) y - f(x_0) = f'(x_0)(x - x_0) y - f(x_0) = f'(x_0)(x - x_0) x_1 perpotongan garis I dengan sumbu - x $$ 0 - f(x_0) = f'(x_0)(x - x_0) $$ y = 0 dan x = x_1 maka koordinat titik ( x_1 ,0) - \\frac{f(x_0)}{f'(x_0)} = (x_1 - x_0) - \\frac{f(x_0)}{f'(x_0)} = (x_1 - x_0) sehingga di dapat sebuah rumus : $$ f'(x_n)=\\frac{f(x_n)-0}{f'(x_n)-{x_{n+1}}} $$ atau dapat diatur kembali menjadi : x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} , x_2 = x_1 - \\frac{f(x_1)}{f'(x_1)}, ... , x_{n+1} = x_{n} - \\frac{f(x_{n})}{f'(x_{n})} x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} , x_2 = x_1 - \\frac{f(x_1)}{f'(x_1)}, ... , x_{n+1} = x_{n} - \\frac{f(x_{n})}{f'(x_{n})} Implementasi Pemrograman \u00b6 Implementasi Newton-Raphson untuk fungsi f(x)= e^x - 4x maka kita membuat turunan untuk fungsi tersebut yaitu f'(x) = e^x - 4 import math e = 2.71828 def fungsi ( x ): x = float (( e ** x ) - ( 4 * x )) return x def fungsiturunan ( x ): x = float (( e ** x ) - ( 4 )) return x x = float ( input ( 'Masukkan nilai awal = ' )) error = float ( input ( 'Masukkan nilai error = ' )) perulangan = int ( input ( 'Masukkan maksimal pengulangan = ' )) iterasi = 0 selisih = error + 1 while iterasi <= perulangan and selisih > error : iterasi += 1 f_2 = x - ( fungsi ( x ) / fungsiturunan ( x )) selisih = math . fabs ( f_2 - x ) x = f_2 print ( \"Iterasi ke = \" , iterasi , \", x = \" , f_2 , \", f(\" , f_2 , \") = \" , fungsi ( f_2 ), \", selisih = \" , error ) if iterasi <= perulangan : print ( \"Perulangan Mencapai Batas Maksimal dengan hasil = \" , f_2 ) else : print ( \"Toleransi tidak terpenuhi\" ) Dengan Output sebagai berikut : Masukkan nilai awal = 0 Masukkan nilai error = 0.0001 Masukkan maksimal pengulangan = 20 Iterasi ke = 1 , x = 0.3333333333333333 , f ( 0.3333333333333333 ) = 0.06227877883196098 , selisih = 0.0001 Perulangan Mencapai Batas Maksimal dengan hasil = 0.3333333333333333 Iterasi ke = 2 , x = 0.35724635301940616 , f ( 0.35724635301940616 ) = 0.0004022049593612742 , selisih = 0.0001 Perulangan Mencapai Batas Maksimal dengan hasil = 0.35724635301940616 Iterasi ke = 3 , x = 0.35740281572145605 , f ( 0.35740281572145605 ) = 1.734656973617632e-08 , selisih = 0.0001 Perulangan Mencapai Batas Maksimal dengan hasil = 0.35740281572145605 Iterasi ke = 4 , x = 0.3574028224700733 , f ( 0.3574028224700733 ) = - 6.439293542825908e-15 , selisih = 0.0001 Perulangan Mencapai Batas Maksimal dengan hasil = 0.3574028224700733 Penjelasan : Importh Library math Membuat sebuah inputan untuk X , Error / Epsilon , serta Maksimal perulangan untuk stopping condition lalu deklarasikan iterasi = 0 untuk perulangan yang ke 0 nantinya dan akan ditambah setiap kali perulangan deklarasikan selisih untuk x_b - x_0 untuk perbandingan lakukan perulangan dengan kondisi iterasi kurang dari sama dengan inputan maksimal iterasi dan selisih lebih dari error / epsilon hitung x_b dengan rumus yang sudah kita dapatkan sebelumnya lalu lakukan perbandingan jika mencapai nilai True maka toleransi tidak terpenuhi namun perulangan sudah mencapai batas jika pengecekan selisih > error bernilai bernilai True maka toleransi akan terpenuhi dengan nilai error serta fungsi x pada iterasi ke n","title":"Newton Raphson"},{"location":"newton-raphson/#newton-raphson","text":"","title":"Newton Raphson"},{"location":"newton-raphson/#newton-raphson_1","text":"Dalam analisis numerik, metode Newton / Newton-Raphson yang mendapat nama dari Isaac Newton dan Joseph Rapshon, merupakan metode yang paling dikenal untuk mencari akar suatu fungsi f(x) dengan pendekatan satu titik dimana fungsi f(x) mempunyai turunan. Metode Newton-Raphson dapat diturunkan berdasarkan interpretasi geometrik (sebuah metode alternatif yang didasarkan pada Deret Taylor ). Prosedur Metode Newton : menentukan x_0 sebagai titik awal, kemudian menarik garis lurus yang menyinggung titik f(x_0) . Hal ini berakibat garis I memotong sumbu x di titik x_1 Setelah itu diulangi langkah sebelumnya tapi sekarang x_1 dianggap sebagai titik awalnya. Dari mengulang langkah-langkah sebelumnya akan mendapatkan x_2 , x_3 , ... , x_n dengan x_n yang diperoleh adalah bilangan riil yang merupakan akar atau mendekati akar yang sebenarnya. persamaan garis I : y - y_0 = m(x - x_0) y - f(x_0) = f'(x_0)(x - x_0) y - f(x_0) = f'(x_0)(x - x_0) x_1 perpotongan garis I dengan sumbu - x $$ 0 - f(x_0) = f'(x_0)(x - x_0) $$ y = 0 dan x = x_1 maka koordinat titik ( x_1 ,0) - \\frac{f(x_0)}{f'(x_0)} = (x_1 - x_0) - \\frac{f(x_0)}{f'(x_0)} = (x_1 - x_0) sehingga di dapat sebuah rumus : $$ f'(x_n)=\\frac{f(x_n)-0}{f'(x_n)-{x_{n+1}}} $$ atau dapat diatur kembali menjadi : x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} , x_2 = x_1 - \\frac{f(x_1)}{f'(x_1)}, ... , x_{n+1} = x_{n} - \\frac{f(x_{n})}{f'(x_{n})} x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} , x_2 = x_1 - \\frac{f(x_1)}{f'(x_1)}, ... , x_{n+1} = x_{n} - \\frac{f(x_{n})}{f'(x_{n})}","title":"Newton-Raphson"},{"location":"newton-raphson/#implementasi-pemrograman","text":"Implementasi Newton-Raphson untuk fungsi f(x)= e^x - 4x maka kita membuat turunan untuk fungsi tersebut yaitu f'(x) = e^x - 4 import math e = 2.71828 def fungsi ( x ): x = float (( e ** x ) - ( 4 * x )) return x def fungsiturunan ( x ): x = float (( e ** x ) - ( 4 )) return x x = float ( input ( 'Masukkan nilai awal = ' )) error = float ( input ( 'Masukkan nilai error = ' )) perulangan = int ( input ( 'Masukkan maksimal pengulangan = ' )) iterasi = 0 selisih = error + 1 while iterasi <= perulangan and selisih > error : iterasi += 1 f_2 = x - ( fungsi ( x ) / fungsiturunan ( x )) selisih = math . fabs ( f_2 - x ) x = f_2 print ( \"Iterasi ke = \" , iterasi , \", x = \" , f_2 , \", f(\" , f_2 , \") = \" , fungsi ( f_2 ), \", selisih = \" , error ) if iterasi <= perulangan : print ( \"Perulangan Mencapai Batas Maksimal dengan hasil = \" , f_2 ) else : print ( \"Toleransi tidak terpenuhi\" ) Dengan Output sebagai berikut : Masukkan nilai awal = 0 Masukkan nilai error = 0.0001 Masukkan maksimal pengulangan = 20 Iterasi ke = 1 , x = 0.3333333333333333 , f ( 0.3333333333333333 ) = 0.06227877883196098 , selisih = 0.0001 Perulangan Mencapai Batas Maksimal dengan hasil = 0.3333333333333333 Iterasi ke = 2 , x = 0.35724635301940616 , f ( 0.35724635301940616 ) = 0.0004022049593612742 , selisih = 0.0001 Perulangan Mencapai Batas Maksimal dengan hasil = 0.35724635301940616 Iterasi ke = 3 , x = 0.35740281572145605 , f ( 0.35740281572145605 ) = 1.734656973617632e-08 , selisih = 0.0001 Perulangan Mencapai Batas Maksimal dengan hasil = 0.35740281572145605 Iterasi ke = 4 , x = 0.3574028224700733 , f ( 0.3574028224700733 ) = - 6.439293542825908e-15 , selisih = 0.0001 Perulangan Mencapai Batas Maksimal dengan hasil = 0.3574028224700733 Penjelasan : Importh Library math Membuat sebuah inputan untuk X , Error / Epsilon , serta Maksimal perulangan untuk stopping condition lalu deklarasikan iterasi = 0 untuk perulangan yang ke 0 nantinya dan akan ditambah setiap kali perulangan deklarasikan selisih untuk x_b - x_0 untuk perbandingan lakukan perulangan dengan kondisi iterasi kurang dari sama dengan inputan maksimal iterasi dan selisih lebih dari error / epsilon hitung x_b dengan rumus yang sudah kita dapatkan sebelumnya lalu lakukan perbandingan jika mencapai nilai True maka toleransi tidak terpenuhi namun perulangan sudah mencapai batas jika pengecekan selisih > error bernilai bernilai True maka toleransi akan terpenuhi dengan nilai error serta fungsi x pada iterasi ke n","title":"Implementasi Pemrograman"},{"location":"recursive-trapezoid/","text":"Recursive Trapezoid Method \u00b6 Aturan trapesium adalah untuk aturan yang digunaka untuk menemukan nilai pasti integral - integral menggunakan metode numerik. Aturan ini terutama didasarkan pada rumus Newton-Cotes yang menyatakan bahwa seseorang dapat menemukan nilai tepat integral sebagai urutan polinomial. Asumsikan bahwa f (x) adalah fungsi kontinu pada interval yang diberikan [a, b]. Sekarang bagilah interval [a, b] menjadi n sub-zona yang sama Metode Recursive Trapezoid 1 interval \u00b6 h=b-a h=b-a R(0,0)=\\frac{b-a}{2}(f(a)+f(b)) R(0,0)=\\frac{b-a}{2}(f(a)+f(b)) Metode Recursive Trapezoid 2 interval \u00b6 h=\\frac{b-a}{2} h=\\frac{b-a}{2} R(1,0)=\\frac{b-a}{2}[(f(a+h)+\\frac{1}{2}f(a)+f(b))] R(1,0)=\\frac{b-a}{2}[(f(a+h)+\\frac{1}{2}f(a)+f(b))] R(1,0)=\\frac{1}{2}R(0,0)+h[f(a+h)] R(1,0)=\\frac{1}{2}R(0,0)+h[f(a+h)] dimana R(0,0) berdasarkan estimasi sebelumnya dan f(a+h) berdasarkan pada titik baru $$ h = \\frac{b-a}{4} $$ R(2,0)=\\frac{b-a}{4}[f(a+h)+f(a+2h)+f(a+3h)+\\frac{1}{2}(f(a)+f(b))] R(2,0)=\\frac{b-a}{4}[f(a+h)+f(a+2h)+f(a+3h)+\\frac{1}{2}(f(a)+f(b))] R(2,0)=\\frac{1}{2}R(1,0)+h[f(a+h)+f(a+3h)] R(2,0)=\\frac{1}{2}R(1,0)+h[f(a+h)+f(a+3h)] dimana R(1,0) berdasarkan estimasi sebelumnya dan f(a+h) berdasarkan titik baru Formula Recursive Trapezoid \u00b6 dari deskripsi diatas maka bisa disimpulkan bahwa rumus recursive trapezoid adalah $$ R(0,0) = \\frac{b-a}{2}[f(a)+f(b)] $$ R(n,0)=\\frac{1}{2}R(n-1,0)+h[\\sum_{k=1}^{2^{(n-1)}}f(a+(2k-1)h)] R(n,0)=\\frac{1}{2}R(n-1,0)+h[\\sum_{k=1}^{2^{(n-1)}}f(a+(2k-1)h)] h = \\frac{b-a}{2^n} h = \\frac{b-a}{2^n} Program \u00b6 #definisi fungsi def fungsi ( x ): y = 1 / ( 1 + x ) return y print ( \"fungsi yang digunakan adalah\" ) print ( \" \\t\\t \" , \"f(x) = 1/(1+x)\" ) print ( \"\" ) a = float ( input ( \"Masukkan batas bawah integral : \" )) b = float ( input ( \"Masukkan batas atas integral : \" )) c = int ( input ( \"masukkan n : \" )) error = [] print ( \"\" ) print ( \"---------HASIL----------\" ) print ( \"iterasi\" , \" \\t \" , \"n\" , \" \\t\\t \" , \"Trapezoid\" ) for iterasi in range ( 0 , c ): n = 2 ** iterasi h = ( b - a ) / n xi = a y = 0 for i in range ( 1 , n ): xi = xi + h y += fungsi ( xi ) trap = (( h ) * ( fungsi ( a ) + ( 2 * y ) + fungsi ( b ))) / 2 error . append ( trap ) print ( iterasi + 1 , \" \\t\\t \" , n , \" \\t\\t \" , trap ) print ( error [ iterasi - 1 ]) print ( error [ iterasi ]) hasil = ( error [ iterasi - 1 ] - error [ iterasi ]) print ( hasil ) print ( \"estimasi error : \" + str ( hasil )) fungsi yang digunakan adalah f(x) = 1/(1+x) Masukkan batas bawah integral : 0 Masukkan batas atas integral : 1 masukkan n : 15 ---------HASIL---------- iterasi n Trapezoid 1 1 0.75 2 2 0.7083333333333333 3 4 0.6970238095238095 4 8 0.6941218503718504 5 16 0.6933912022075267 6 32 0.6932082082692488 7 64 0.6931624388834033 8 128 0.6931509952281075 9 256 0.6931481342324433 10 512 0.6931474189784099 11 1024 0.6931472401645831 12 2048 0.6931471954611083 13 4096 0.6931471842852348 14 8192 0.69314718149127 15 16384 0.6931471807927745 0.69314718149127 0.6931471807927745 6.984954836752877e-10 estimasi error : 6.984954836752877e-10","title":"Recursive Trapezoid Rule"},{"location":"recursive-trapezoid/#recursive-trapezoid-method","text":"Aturan trapesium adalah untuk aturan yang digunaka untuk menemukan nilai pasti integral - integral menggunakan metode numerik. Aturan ini terutama didasarkan pada rumus Newton-Cotes yang menyatakan bahwa seseorang dapat menemukan nilai tepat integral sebagai urutan polinomial. Asumsikan bahwa f (x) adalah fungsi kontinu pada interval yang diberikan [a, b]. Sekarang bagilah interval [a, b] menjadi n sub-zona yang sama","title":"Recursive Trapezoid Method"},{"location":"recursive-trapezoid/#metode-recursive-trapezoid-1-interval","text":"h=b-a h=b-a R(0,0)=\\frac{b-a}{2}(f(a)+f(b)) R(0,0)=\\frac{b-a}{2}(f(a)+f(b))","title":"Metode Recursive Trapezoid 1 interval"},{"location":"recursive-trapezoid/#metode-recursive-trapezoid-2-interval","text":"h=\\frac{b-a}{2} h=\\frac{b-a}{2} R(1,0)=\\frac{b-a}{2}[(f(a+h)+\\frac{1}{2}f(a)+f(b))] R(1,0)=\\frac{b-a}{2}[(f(a+h)+\\frac{1}{2}f(a)+f(b))] R(1,0)=\\frac{1}{2}R(0,0)+h[f(a+h)] R(1,0)=\\frac{1}{2}R(0,0)+h[f(a+h)] dimana R(0,0) berdasarkan estimasi sebelumnya dan f(a+h) berdasarkan pada titik baru $$ h = \\frac{b-a}{4} $$ R(2,0)=\\frac{b-a}{4}[f(a+h)+f(a+2h)+f(a+3h)+\\frac{1}{2}(f(a)+f(b))] R(2,0)=\\frac{b-a}{4}[f(a+h)+f(a+2h)+f(a+3h)+\\frac{1}{2}(f(a)+f(b))] R(2,0)=\\frac{1}{2}R(1,0)+h[f(a+h)+f(a+3h)] R(2,0)=\\frac{1}{2}R(1,0)+h[f(a+h)+f(a+3h)] dimana R(1,0) berdasarkan estimasi sebelumnya dan f(a+h) berdasarkan titik baru","title":"Metode Recursive Trapezoid 2 interval"},{"location":"recursive-trapezoid/#formula-recursive-trapezoid","text":"dari deskripsi diatas maka bisa disimpulkan bahwa rumus recursive trapezoid adalah $$ R(0,0) = \\frac{b-a}{2}[f(a)+f(b)] $$ R(n,0)=\\frac{1}{2}R(n-1,0)+h[\\sum_{k=1}^{2^{(n-1)}}f(a+(2k-1)h)] R(n,0)=\\frac{1}{2}R(n-1,0)+h[\\sum_{k=1}^{2^{(n-1)}}f(a+(2k-1)h)] h = \\frac{b-a}{2^n} h = \\frac{b-a}{2^n}","title":"Formula Recursive Trapezoid"},{"location":"recursive-trapezoid/#program","text":"#definisi fungsi def fungsi ( x ): y = 1 / ( 1 + x ) return y print ( \"fungsi yang digunakan adalah\" ) print ( \" \\t\\t \" , \"f(x) = 1/(1+x)\" ) print ( \"\" ) a = float ( input ( \"Masukkan batas bawah integral : \" )) b = float ( input ( \"Masukkan batas atas integral : \" )) c = int ( input ( \"masukkan n : \" )) error = [] print ( \"\" ) print ( \"---------HASIL----------\" ) print ( \"iterasi\" , \" \\t \" , \"n\" , \" \\t\\t \" , \"Trapezoid\" ) for iterasi in range ( 0 , c ): n = 2 ** iterasi h = ( b - a ) / n xi = a y = 0 for i in range ( 1 , n ): xi = xi + h y += fungsi ( xi ) trap = (( h ) * ( fungsi ( a ) + ( 2 * y ) + fungsi ( b ))) / 2 error . append ( trap ) print ( iterasi + 1 , \" \\t\\t \" , n , \" \\t\\t \" , trap ) print ( error [ iterasi - 1 ]) print ( error [ iterasi ]) hasil = ( error [ iterasi - 1 ] - error [ iterasi ]) print ( hasil ) print ( \"estimasi error : \" + str ( hasil )) fungsi yang digunakan adalah f(x) = 1/(1+x) Masukkan batas bawah integral : 0 Masukkan batas atas integral : 1 masukkan n : 15 ---------HASIL---------- iterasi n Trapezoid 1 1 0.75 2 2 0.7083333333333333 3 4 0.6970238095238095 4 8 0.6941218503718504 5 16 0.6933912022075267 6 32 0.6932082082692488 7 64 0.6931624388834033 8 128 0.6931509952281075 9 256 0.6931481342324433 10 512 0.6931474189784099 11 1024 0.6931472401645831 12 2048 0.6931471954611083 13 4096 0.6931471842852348 14 8192 0.69314718149127 15 16384 0.6931471807927745 0.69314718149127 0.6931471807927745 6.984954836752877e-10 estimasi error : 6.984954836752877e-10","title":"Program"},{"location":"richardson-extrapolation/","text":"Ekstrapolasi Richardson \u00b6 Ekstrapolasi Richardson merupakan metode yang menggunakan dua perkiraan dari sebuah integral untuk mengkomputasi pendugaan ketiga, yang lebih akurat. Tujuan ekstrapolasi Richardson ialah menghitung nilai integrasi yang lebih baik (improve) dibandingkan dengan I. Misalkan J adalah nilai integrasi yang lebih baik daripada I dengan jarak antar titik adalah h: $$ J = I(h) + Ch^q $$ Ekstrapolasikan h menjadi 2h, lalu hitung integrasi numeriknya : $$ J = I (2h) + C(2h)^q $$ Eliminasikan C dari kedua persamaan dengan menyamakan persamaan (1) dan persamaan (2): $$ I(h) = Ch^q = I(2h) + C(2h)^q $$ sehingga diperoleh $$ C = \\frac{I(h)-I(2h)}{(2 q-1)h q} $$ Program \u00b6 from math import * def zeros ( n , m ): Z = [] for i in range ( n ): Z . append ([ 0 ] * m ) return Z def D ( Func , a , h ): return ( Func ( a + h ) - Func ( a - h )) / ( 2 * h ) def Richardson_dif ( func , a ): '''Richardson extrapolation method for numerical calculation of first derivative ''' k = 9 L = zeros ( k , k ) for I in range ( k ): L [ I ][ 0 ] = D ( func , a , 1 / ( 2 ** ( I + 1 ))) for j in range ( 1 , k ): for i in range ( k - j ): L [ i ][ j ] = (( 4 ** ( j )) * L [ i + 1 ][ j - 1 ] - L [ i ][ j - 1 ]) / ( 4 ** ( j ) - 1 ) return L [ 0 ][ k - 1 ] print ( '>>>>>>>>>>>>>>>>>>>>>>> DIFERENSIASI NUMERIK DARI <<<<<<<<<<<<<<<<<<<<<' ) print ( \"=======================================================================\" ) print ( 'f = -0.1*x**4-0.15*x**3-0.5*x**2-0.25*x+1.2 dengan x = 0.5' ) print ( \"=======================================================================\" ) print ( ' %04.20f ' % Richardson_dif ( lambda x : - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 , 0.5 )) print ( \"=======================================================================\" ) print ( 'diff(2**cos(pi+sin(x)) dengan x = pi/2 adalah = %04.20f ' % Richardson_dif ( lambda x : 2 ** cos ( pi + sin ( x )), pi / 3 )) Hasilnya >>>>>>>>>>>>>>>>>>>>>>> DIFERENSIASI NUMERIK DARI <<<<<<<<<<<<<<<<<<<<< ======================================================================= f = - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 dengan x = 0.5 ======================================================================= - 0.91250000000000530687 ======================================================================= diff ( 2 ** cos ( pi + sin ( x )) dengan x = pi / 2 adalah = 0.16849558398154249050 >>>","title":"Richardson Extrapolation"},{"location":"richardson-extrapolation/#ekstrapolasi-richardson","text":"Ekstrapolasi Richardson merupakan metode yang menggunakan dua perkiraan dari sebuah integral untuk mengkomputasi pendugaan ketiga, yang lebih akurat. Tujuan ekstrapolasi Richardson ialah menghitung nilai integrasi yang lebih baik (improve) dibandingkan dengan I. Misalkan J adalah nilai integrasi yang lebih baik daripada I dengan jarak antar titik adalah h: $$ J = I(h) + Ch^q $$ Ekstrapolasikan h menjadi 2h, lalu hitung integrasi numeriknya : $$ J = I (2h) + C(2h)^q $$ Eliminasikan C dari kedua persamaan dengan menyamakan persamaan (1) dan persamaan (2): $$ I(h) = Ch^q = I(2h) + C(2h)^q $$ sehingga diperoleh $$ C = \\frac{I(h)-I(2h)}{(2 q-1)h q} $$","title":"Ekstrapolasi Richardson"},{"location":"richardson-extrapolation/#program","text":"from math import * def zeros ( n , m ): Z = [] for i in range ( n ): Z . append ([ 0 ] * m ) return Z def D ( Func , a , h ): return ( Func ( a + h ) - Func ( a - h )) / ( 2 * h ) def Richardson_dif ( func , a ): '''Richardson extrapolation method for numerical calculation of first derivative ''' k = 9 L = zeros ( k , k ) for I in range ( k ): L [ I ][ 0 ] = D ( func , a , 1 / ( 2 ** ( I + 1 ))) for j in range ( 1 , k ): for i in range ( k - j ): L [ i ][ j ] = (( 4 ** ( j )) * L [ i + 1 ][ j - 1 ] - L [ i ][ j - 1 ]) / ( 4 ** ( j ) - 1 ) return L [ 0 ][ k - 1 ] print ( '>>>>>>>>>>>>>>>>>>>>>>> DIFERENSIASI NUMERIK DARI <<<<<<<<<<<<<<<<<<<<<' ) print ( \"=======================================================================\" ) print ( 'f = -0.1*x**4-0.15*x**3-0.5*x**2-0.25*x+1.2 dengan x = 0.5' ) print ( \"=======================================================================\" ) print ( ' %04.20f ' % Richardson_dif ( lambda x : - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 , 0.5 )) print ( \"=======================================================================\" ) print ( 'diff(2**cos(pi+sin(x)) dengan x = pi/2 adalah = %04.20f ' % Richardson_dif ( lambda x : 2 ** cos ( pi + sin ( x )), pi / 3 )) Hasilnya >>>>>>>>>>>>>>>>>>>>>>> DIFERENSIASI NUMERIK DARI <<<<<<<<<<<<<<<<<<<<< ======================================================================= f = - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 dengan x = 0.5 ======================================================================= - 0.91250000000000530687 ======================================================================= diff ( 2 ** cos ( pi + sin ( x )) dengan x = pi / 2 adalah = 0.16849558398154249050 >>>","title":"Program"},{"location":"seleksi-fitur/","text":"Seleksi Fitur adalah kegiatan untuk memilih / menyaring / memberi peringkat terhadap tiap tiap fitur dalam data agar nantinya fitur yang memiliki peringkat rendah dapat dibuang Seleksi Fitur sangat berguna dalam Data Mining karena dapat menghemat waktu, biaya, dan tenaga dengan cara membuang fitur yang tidak penting. jika kita menganalisa banyak kolom dengan kita menyeleksi fitur data tersebut kita dapat mengurangi kolom data itu sehingga membuat waktu analisis lebih cepat Information Gain \u00b6 Ada banyak teknik untuk menyeleksi fitur. Information Gain bekerja dengan mendeteksi fitur yang paling banyak memiliki informasi berdasarkan kelas tertentu dengan menghitung nilai Entropy-nya Rumus untuk mencari Entropy Target adalah: $$ Entropy(S):\\sum_{i}^{c}-P_{i}\\log_{2}P_{i} $$ Dimana c adalah jumlah nilai yang ada pada kelas kategori dan Pi adalah jumlah sampel untuk kelas i from pandas import * from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) df = read_csv ( 'newdata.csv' , sep = ';' ) table ( df ) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no Mencari Entropy Target \u00b6 def findEntropy ( column ): rawGroups = df . groupby ( column ) targetGroups = [[ key , len ( data ), len ( data ) / df [ column ] . size ] for key , data in rawGroups ] targetGroups = DataFrame ( targetGroups , columns = [ 'value' , 'count' , 'probability' ]) return sum ([ - x * log ( x , 2 ) for x in targetGroups [ 'probability' ]]), targetGroups , rawGroups entropyTarget , groupTargets , _ = findEntropy ( 'play' ) table ( groupTargets ) print ( 'entropy target =' , entropyTarget ) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309 Dari atas kita tahu bahwa Entropy Target adalah hasil dari penjumlahan Entropy kelas no dan yes Kemudian untuk mencari Entropy setiap fitur / atribut dalam data digunakan rumus: $$ Gain(S,A)=Entropy(S)-\\sum_{values(A)}\\frac{|S_{v}|}{|S|}Entropy(S_{v}) $$ A adalah Atribut dan |Sv| adalah jumlah sampel untuk nilai v, |S| adalah jumlah seluruh sampel serta Entropy(Sv) adalah Entropy dari sampel yang memiliki nilai v Rumus ini digunakan untuk semua kolom def findGain ( column ): entropyOutlook , groupOutlooks , rawOutlooks = findEntropy ( column ) table ( groupOutlooks ) gain = entropyTarget - sum ( len ( data ) / len ( df ) * sum ( - x / len ( data ) * log ( x / len ( data ), 2 ) for x in data . groupby ( 'play' ) . size ()) for key , data in rawOutlooks ) print ( \"gain of\" , column , \"is\" , gain ) return gain gains = [[ x , findGain ( x )] for x in [ 'outlook' , 'temperature' , 'humidity' , 'windy' ]] value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927 Hasil daripada setiap kolomnya: table ( DataFrame ( gains , columns = [ \"Feature\" , \"Gain Score\" ]) . sort_values ( \"Gain Score\" )[:: - 1 ]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 Setelah mengetahui hasil dari setiap kolom, maka akan terlihat mana kolom yang memiliki hasil tertinggi dan terendah. Karena kita sudah mengetahui kolom dengan nilai yang terendah adalah temperatur maka tidak apa apa untuk menghilangkan kolom temperatur dari data Referensi \u00b6 https://edrianhadinata.wordpress.com/tag/reduksi-dimensi/ https://www.researchgate.net/publication/326571453_Seleksi_Fitur_Information_Gain_untuk_Klasifikasi_Penyakit_Jantung_Menggunakan_Kombinasi_Metode_K-Nearest_Neighbor_dan_Naive_Bayes https://informatikalogi.com/algoritma-id3","title":"Seleksi Fitur"},{"location":"seleksi-fitur/#information-gain","text":"Ada banyak teknik untuk menyeleksi fitur. Information Gain bekerja dengan mendeteksi fitur yang paling banyak memiliki informasi berdasarkan kelas tertentu dengan menghitung nilai Entropy-nya Rumus untuk mencari Entropy Target adalah: $$ Entropy(S):\\sum_{i}^{c}-P_{i}\\log_{2}P_{i} $$ Dimana c adalah jumlah nilai yang ada pada kelas kategori dan Pi adalah jumlah sampel untuk kelas i from pandas import * from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) df = read_csv ( 'newdata.csv' , sep = ';' ) table ( df ) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no","title":"Information Gain"},{"location":"seleksi-fitur/#mencari-entropy-target","text":"def findEntropy ( column ): rawGroups = df . groupby ( column ) targetGroups = [[ key , len ( data ), len ( data ) / df [ column ] . size ] for key , data in rawGroups ] targetGroups = DataFrame ( targetGroups , columns = [ 'value' , 'count' , 'probability' ]) return sum ([ - x * log ( x , 2 ) for x in targetGroups [ 'probability' ]]), targetGroups , rawGroups entropyTarget , groupTargets , _ = findEntropy ( 'play' ) table ( groupTargets ) print ( 'entropy target =' , entropyTarget ) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309 Dari atas kita tahu bahwa Entropy Target adalah hasil dari penjumlahan Entropy kelas no dan yes Kemudian untuk mencari Entropy setiap fitur / atribut dalam data digunakan rumus: $$ Gain(S,A)=Entropy(S)-\\sum_{values(A)}\\frac{|S_{v}|}{|S|}Entropy(S_{v}) $$ A adalah Atribut dan |Sv| adalah jumlah sampel untuk nilai v, |S| adalah jumlah seluruh sampel serta Entropy(Sv) adalah Entropy dari sampel yang memiliki nilai v Rumus ini digunakan untuk semua kolom def findGain ( column ): entropyOutlook , groupOutlooks , rawOutlooks = findEntropy ( column ) table ( groupOutlooks ) gain = entropyTarget - sum ( len ( data ) / len ( df ) * sum ( - x / len ( data ) * log ( x / len ( data ), 2 ) for x in data . groupby ( 'play' ) . size ()) for key , data in rawOutlooks ) print ( \"gain of\" , column , \"is\" , gain ) return gain gains = [[ x , findGain ( x )] for x in [ 'outlook' , 'temperature' , 'humidity' , 'windy' ]] value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927 Hasil daripada setiap kolomnya: table ( DataFrame ( gains , columns = [ \"Feature\" , \"Gain Score\" ]) . sort_values ( \"Gain Score\" )[:: - 1 ]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 Setelah mengetahui hasil dari setiap kolom, maka akan terlihat mana kolom yang memiliki hasil tertinggi dan terendah. Karena kita sudah mengetahui kolom dengan nilai yang terendah adalah temperatur maka tidak apa apa untuk menghilangkan kolom temperatur dari data","title":"Mencari Entropy Target"},{"location":"seleksi-fitur/#referensi","text":"https://edrianhadinata.wordpress.com/tag/reduksi-dimensi/ https://www.researchgate.net/publication/326571453_Seleksi_Fitur_Information_Gain_untuk_Klasifikasi_Penyakit_Jantung_Menggunakan_Kombinasi_Metode_K-Nearest_Neighbor_dan_Naive_Bayes https://informatikalogi.com/algoritma-id3","title":"Referensi"},{"location":"soal-naive-bayes/","text":"Teorema Bayes : \u00b6 Tipe Data Numerik \u00b6 $$ P(c|x) = \\frac{P(x_i|c) P\u00a9}{P(x_i)} $$ Dimana (P\u00a9) = Prior/Probabilitas kelas dari data yang ada P(c) P(c) = Prior (Probability) P(xi) P(xi) = Evidenence dari setiap fitur/Probabilitas dari setiap fitur (P(xi|c) = Likelihood dari setiap fitur yang diperoleh dari setiap kelas dengan menggunakan rumus: $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e {-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c}) 2} $$ Tipe Data Categorical \u00b6 P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} Dimana P(xi|c) dapat diperoleh dari probabilitas berapa banyak fitur yang muncul dibagi banyak kelas yang muncul pada data yang ada Tipe Data Campuran \u00b6 Untuk tipe data campuran maka kita akan menggunakan rumus sesuai tipe dari attribut tersebut. Jika atributnya adalah numerik maka kita akan menggunakan rumus numerik akan tetapi jika attributnya categorical maka kita akan meggunakan rumus categorical Contoh Soal: outlook temperature humidity windy play sunny 85 high FALSE no sunny 80 high TRUE no overcast 83 high FALSE yes rainy 70 high FALSE yes rainy 68 normal FALSE yes rainy 65 normal TRUE no overcast 64 normal TRUE yes sunny 72 high FALSE no sunny 69 normal FALSE yes rainy 75 normal FALSE yes sunny 75 normal TRUE yes overcast 72 high TRUE yes overcast 81 normal FALSE yes rainy 71 high TRUE no rainy 60 high FALSE ??? Untuk menentukan/menebak kelas dari data yang baru kita akan memakai teknik naive bayes. Dan didalam data baru tersebut terdapat atribut categorical dan atribut numerik maka untuk atribut categorical kita menggunakan probabilitas atribut itu muncul dalam setiap kelas dan untuk data numerik kita menggunakan rumus : $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e {-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c}) 2} $$ untuk menghitung rata rata dan standart deviasi dari atribut numerik 60 kita bisa menggunakan excel dan memperoleh hasil Rata - Rata = 72,66667 Standard Deviasi = 7,237469 P(60|C) = 4.027899533 untuk probabilitas kelas YES yaitu : 9/14 = 0,642857 untuk probabilitas kelas NO yaitu : 5/14 = 0,357143 untuk fitur categorical Rainy Yes : 3/9 = 0,333333 No : \u2156 = 0,4 Humidity Yes : 3/9 = 0,333333 No : \u2158 = 0,8 False Yes : 6/9 = 0,666667 No : \u2156 = 0,4 P(Yes|x) = 0.1918044093 P(No|x) = 0.1841326237 Maka kita bisa menebak bahwa data baru tersebut memiliki kelas YES outlook temperature humidity windy play rainy 60 high FALSE yes","title":"Soal Naive Bayes"},{"location":"soal-naive-bayes/#teorema-bayes","text":"","title":"Teorema Bayes :"},{"location":"soal-naive-bayes/#tipe-data-numerik","text":"$$ P(c|x) = \\frac{P(x_i|c) P\u00a9}{P(x_i)} $$ Dimana (P\u00a9) = Prior/Probabilitas kelas dari data yang ada P(c) P(c) = Prior (Probability) P(xi) P(xi) = Evidenence dari setiap fitur/Probabilitas dari setiap fitur (P(xi|c) = Likelihood dari setiap fitur yang diperoleh dari setiap kelas dengan menggunakan rumus: $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e {-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c}) 2} $$","title":"Tipe Data Numerik"},{"location":"soal-naive-bayes/#tipe-data-categorical","text":"P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} P(c|x) = \\frac{P(x_i|c) P(c)}{P(x_i)} Dimana P(xi|c) dapat diperoleh dari probabilitas berapa banyak fitur yang muncul dibagi banyak kelas yang muncul pada data yang ada","title":"Tipe Data Categorical"},{"location":"soal-naive-bayes/#tipe-data-campuran","text":"Untuk tipe data campuran maka kita akan menggunakan rumus sesuai tipe dari attribut tersebut. Jika atributnya adalah numerik maka kita akan menggunakan rumus numerik akan tetapi jika attributnya categorical maka kita akan meggunakan rumus categorical Contoh Soal: outlook temperature humidity windy play sunny 85 high FALSE no sunny 80 high TRUE no overcast 83 high FALSE yes rainy 70 high FALSE yes rainy 68 normal FALSE yes rainy 65 normal TRUE no overcast 64 normal TRUE yes sunny 72 high FALSE no sunny 69 normal FALSE yes rainy 75 normal FALSE yes sunny 75 normal TRUE yes overcast 72 high TRUE yes overcast 81 normal FALSE yes rainy 71 high TRUE no rainy 60 high FALSE ??? Untuk menentukan/menebak kelas dari data yang baru kita akan memakai teknik naive bayes. Dan didalam data baru tersebut terdapat atribut categorical dan atribut numerik maka untuk atribut categorical kita menggunakan probabilitas atribut itu muncul dalam setiap kelas dan untuk data numerik kita menggunakan rumus : $$ P(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\sigma_c}} e {-\\frac{1}{2}(\\frac{x_i-\\mu_c}{\\sigma_c}) 2} $$ untuk menghitung rata rata dan standart deviasi dari atribut numerik 60 kita bisa menggunakan excel dan memperoleh hasil Rata - Rata = 72,66667 Standard Deviasi = 7,237469 P(60|C) = 4.027899533 untuk probabilitas kelas YES yaitu : 9/14 = 0,642857 untuk probabilitas kelas NO yaitu : 5/14 = 0,357143 untuk fitur categorical Rainy Yes : 3/9 = 0,333333 No : \u2156 = 0,4 Humidity Yes : 3/9 = 0,333333 No : \u2158 = 0,8 False Yes : 6/9 = 0,666667 No : \u2156 = 0,4 P(Yes|x) = 0.1918044093 P(No|x) = 0.1841326237 Maka kita bisa menebak bahwa data baru tersebut memiliki kelas YES outlook temperature humidity windy play rainy 60 high FALSE yes","title":"Tipe Data Campuran"}]}